{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T08:22:00.963633Z",
     "start_time": "2020-07-19T08:22:00.949854Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "import re \n",
    "import time\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T08:22:16.606099Z",
     "start_time": "2020-07-19T08:22:01.841542Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T08:22:27.620484Z",
     "start_time": "2020-07-19T08:22:16.610036Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\gaura\\\\Desktop\\\\Modern Natural Language Processing in Python\\\\Transformer\\\\europarl-v7.fr-en.en\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "with open(\"C:\\\\Users\\\\gaura\\\\Desktop\\\\Modern Natural Language Processing in Python\\\\Transformer\\\\europarl-v7.fr-en.fr\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    europarl_fr = f.read()\n",
    "with open(\"C:\\\\Users\\\\gaura\\\\Desktop\\\\Modern Natural Language Processing in Python\\\\Transformer\\\\nonbreaking_prefix.en\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "with open(\"C:\\\\Users\\\\gaura\\\\Desktop\\\\Modern Natural Language Processing in Python\\\\Transformer\\\\nonbreaking_prefix.fr\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    non_breaking_prefix_fr = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T08:22:27.680725Z",
     "start_time": "2020-07-19T08:22:27.639726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the se'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "europarl_en[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T08:22:54.727590Z",
     "start_time": "2020-07-19T08:22:54.699801Z"
    }
   },
   "outputs": [],
   "source": [
    "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
    "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
    "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
    "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T08:25:14.932486Z",
     "start_time": "2020-07-19T08:22:56.182470Z"
    }
   },
   "outputs": [],
   "source": [
    "#We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so\n",
    "#we can \"tokenize\" them.\n",
    "corpus_en = europarl_en\n",
    "# Add $$$ after non ending sentence points\n",
    "for prefix in non_breaking_prefix_en:\n",
    "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "# Remove $$$ markers\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "# Clear multiple spaces\n",
    "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "corpus_fr = europarl_fr\n",
    "for prefix in non_breaking_prefix_fr:\n",
    "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
    "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
    "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
    "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
    "corpus_fr = corpus_fr.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:03:56.509786Z",
     "start_time": "2020-07-19T08:26:59.308641Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfdf.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_en, target_vocab_size=2**13)\n",
    "tokenizer_fr = tfdf.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_fr, target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
    "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2\n",
    "\n",
    "\n",
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
    "           for sentence in corpus_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove too long sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:16:40.690630Z",
     "start_time": "2020-07-19T09:03:56.529915Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:20:58.892534Z",
     "start_time": "2020-07-19T09:20:44.414789Z"
    }
   },
   "outputs": [],
   "source": [
    "#Input and Output Creation\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:20:59.648938Z",
     "start_time": "2020-07-19T09:20:58.899201Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:21:05.482842Z",
     "start_time": "2020-07-19T09:21:05.422946Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:21:06.768612Z",
     "start_time": "2020-07-19T09:21:06.746838Z"
    }
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    \n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:21:08.168220Z",
     "start_time": "2020-07-19T09:21:08.128503Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "        \n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "        \n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        \n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        \n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:21:40.515570Z",
     "start_time": "2020-07-19T09:21:40.494183Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:21:51.261376Z",
     "start_time": "2020-07-19T09:21:51.240296Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:22:16.266596Z",
     "start_time": "2020-07-19T09:22:16.231424Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention combined with encoder output\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:22:25.338571Z",
     "start_time": "2020-07-19T09:22:25.284557Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for i in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:22:49.159182Z",
     "start_time": "2020-07-19T09:22:49.120204Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        \n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        \n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:25:07.415200Z",
     "start_time": "2020-07-19T09:25:07.015830Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:25:15.723142Z",
     "start_time": "2020-07-19T09:25:15.623939Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:25:24.544701Z",
     "start_time": "2020-07-19T09:25:24.498962Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T09:26:26.065202Z",
     "start_time": "2020-07-19T09:26:25.995549Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"C:\\\\Users\\\\gaura\\\\Desktop\\\\Modern Natural Language Processing in Python\\\\Transformer\\\\ckpt\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T05:30:26.448564Z",
     "start_time": "2020-07-19T09:26:38.500917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Batch 0 Loss 6.6340 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 6.2636 Accuracy 0.0101\n",
      "Epoch 1 Batch 100 Loss 6.1788 Accuracy 0.0311\n",
      "Epoch 1 Batch 150 Loss 6.1183 Accuracy 0.0383\n",
      "Epoch 1 Batch 200 Loss 6.0342 Accuracy 0.0418\n",
      "Epoch 1 Batch 250 Loss 5.9366 Accuracy 0.0440\n",
      "Epoch 1 Batch 300 Loss 5.8097 Accuracy 0.0455\n",
      "Epoch 1 Batch 350 Loss 5.6868 Accuracy 0.0500\n",
      "Epoch 1 Batch 400 Loss 5.5521 Accuracy 0.0552\n",
      "Epoch 1 Batch 450 Loss 5.4314 Accuracy 0.0606\n",
      "Epoch 1 Batch 500 Loss 5.3187 Accuracy 0.0664\n",
      "Epoch 1 Batch 550 Loss 5.2144 Accuracy 0.0726\n",
      "Epoch 1 Batch 600 Loss 5.1134 Accuracy 0.0785\n",
      "Epoch 1 Batch 650 Loss 5.0207 Accuracy 0.0841\n",
      "Epoch 1 Batch 700 Loss 4.9302 Accuracy 0.0895\n",
      "Epoch 1 Batch 750 Loss 4.8453 Accuracy 0.0946\n",
      "Epoch 1 Batch 800 Loss 4.7643 Accuracy 0.0996\n",
      "Epoch 1 Batch 850 Loss 4.6867 Accuracy 0.1046\n",
      "Epoch 1 Batch 900 Loss 4.6147 Accuracy 0.1094\n",
      "Epoch 1 Batch 950 Loss 4.5478 Accuracy 0.1141\n",
      "Epoch 1 Batch 1000 Loss 4.4831 Accuracy 0.1186\n",
      "Epoch 1 Batch 1050 Loss 4.4227 Accuracy 0.1230\n",
      "Epoch 1 Batch 1100 Loss 4.3648 Accuracy 0.1269\n",
      "Epoch 1 Batch 1150 Loss 4.3116 Accuracy 0.1304\n",
      "Epoch 1 Batch 1200 Loss 4.2624 Accuracy 0.1339\n",
      "Epoch 1 Batch 1250 Loss 4.2135 Accuracy 0.1372\n",
      "Epoch 1 Batch 1300 Loss 4.1680 Accuracy 0.1404\n",
      "Epoch 1 Batch 1350 Loss 4.1245 Accuracy 0.1434\n",
      "Epoch 1 Batch 1400 Loss 4.0829 Accuracy 0.1465\n",
      "Epoch 1 Batch 1450 Loss 4.0431 Accuracy 0.1496\n",
      "Epoch 1 Batch 1500 Loss 4.0034 Accuracy 0.1526\n",
      "Epoch 1 Batch 1550 Loss 3.9670 Accuracy 0.1556\n",
      "Epoch 1 Batch 1600 Loss 3.9308 Accuracy 0.1585\n",
      "Epoch 1 Batch 1650 Loss 3.8968 Accuracy 0.1612\n",
      "Epoch 1 Batch 1700 Loss 3.8626 Accuracy 0.1639\n",
      "Epoch 1 Batch 1750 Loss 3.8295 Accuracy 0.1667\n",
      "Epoch 1 Batch 1800 Loss 3.7992 Accuracy 0.1692\n",
      "Epoch 1 Batch 1850 Loss 3.7701 Accuracy 0.1718\n",
      "Epoch 1 Batch 1900 Loss 3.7413 Accuracy 0.1744\n",
      "Epoch 1 Batch 1950 Loss 3.7126 Accuracy 0.1768\n",
      "Epoch 1 Batch 2000 Loss 3.6856 Accuracy 0.1792\n",
      "Epoch 1 Batch 2050 Loss 3.6590 Accuracy 0.1814\n",
      "Epoch 1 Batch 2100 Loss 3.6313 Accuracy 0.1834\n",
      "Epoch 1 Batch 2150 Loss 3.6055 Accuracy 0.1853\n",
      "Epoch 1 Batch 2200 Loss 3.5782 Accuracy 0.1872\n",
      "Epoch 1 Batch 2250 Loss 3.5528 Accuracy 0.1892\n",
      "Epoch 1 Batch 2300 Loss 3.5263 Accuracy 0.1911\n",
      "Epoch 1 Batch 2350 Loss 3.5008 Accuracy 0.1930\n",
      "Epoch 1 Batch 2400 Loss 3.4763 Accuracy 0.1949\n",
      "Epoch 1 Batch 2450 Loss 3.4529 Accuracy 0.1969\n",
      "Epoch 1 Batch 2500 Loss 3.4296 Accuracy 0.1989\n",
      "Epoch 1 Batch 2550 Loss 3.4068 Accuracy 0.2009\n",
      "Epoch 1 Batch 2600 Loss 3.3842 Accuracy 0.2028\n",
      "Epoch 1 Batch 2650 Loss 3.3622 Accuracy 0.2047\n",
      "Epoch 1 Batch 2700 Loss 3.3412 Accuracy 0.2067\n",
      "Epoch 1 Batch 2750 Loss 3.3198 Accuracy 0.2087\n",
      "Epoch 1 Batch 2800 Loss 3.2992 Accuracy 0.2107\n",
      "Epoch 1 Batch 2850 Loss 3.2785 Accuracy 0.2127\n",
      "Epoch 1 Batch 2900 Loss 3.2582 Accuracy 0.2146\n",
      "Epoch 1 Batch 2950 Loss 3.2382 Accuracy 0.2164\n",
      "Epoch 1 Batch 3000 Loss 3.2183 Accuracy 0.2183\n",
      "Epoch 1 Batch 3050 Loss 3.1997 Accuracy 0.2202\n",
      "Epoch 1 Batch 3100 Loss 3.1812 Accuracy 0.2220\n",
      "Epoch 1 Batch 3150 Loss 3.1626 Accuracy 0.2239\n",
      "Epoch 1 Batch 3200 Loss 3.1442 Accuracy 0.2257\n",
      "Epoch 1 Batch 3250 Loss 3.1258 Accuracy 0.2276\n",
      "Epoch 1 Batch 3300 Loss 3.1074 Accuracy 0.2294\n",
      "Epoch 1 Batch 3350 Loss 3.0900 Accuracy 0.2312\n",
      "Epoch 1 Batch 3400 Loss 3.0725 Accuracy 0.2330\n",
      "Epoch 1 Batch 3450 Loss 3.0555 Accuracy 0.2348\n",
      "Epoch 1 Batch 3500 Loss 3.0386 Accuracy 0.2366\n",
      "Epoch 1 Batch 3550 Loss 3.0218 Accuracy 0.2384\n",
      "Epoch 1 Batch 3600 Loss 3.0056 Accuracy 0.2401\n",
      "Epoch 1 Batch 3650 Loss 2.9895 Accuracy 0.2417\n",
      "Epoch 1 Batch 3700 Loss 2.9742 Accuracy 0.2435\n",
      "Epoch 1 Batch 3750 Loss 2.9588 Accuracy 0.2453\n",
      "Epoch 1 Batch 3800 Loss 2.9435 Accuracy 0.2470\n",
      "Epoch 1 Batch 3850 Loss 2.9289 Accuracy 0.2486\n",
      "Epoch 1 Batch 3900 Loss 2.9144 Accuracy 0.2503\n",
      "Epoch 1 Batch 3950 Loss 2.8998 Accuracy 0.2520\n",
      "Epoch 1 Batch 4000 Loss 2.8860 Accuracy 0.2537\n",
      "Epoch 1 Batch 4050 Loss 2.8721 Accuracy 0.2553\n",
      "Epoch 1 Batch 4100 Loss 2.8584 Accuracy 0.2568\n",
      "Epoch 1 Batch 4150 Loss 2.8457 Accuracy 0.2583\n",
      "Epoch 1 Batch 4200 Loss 2.8338 Accuracy 0.2596\n",
      "Epoch 1 Batch 4250 Loss 2.8222 Accuracy 0.2609\n",
      "Epoch 1 Batch 4300 Loss 2.8105 Accuracy 0.2622\n",
      "Epoch 1 Batch 4350 Loss 2.7996 Accuracy 0.2634\n",
      "Epoch 1 Batch 4400 Loss 2.7889 Accuracy 0.2646\n",
      "Epoch 1 Batch 4450 Loss 2.7786 Accuracy 0.2657\n",
      "Epoch 1 Batch 4500 Loss 2.7683 Accuracy 0.2668\n",
      "Epoch 1 Batch 4550 Loss 2.7579 Accuracy 0.2679\n",
      "Epoch 1 Batch 4600 Loss 2.7484 Accuracy 0.2690\n",
      "Epoch 1 Batch 4650 Loss 2.7386 Accuracy 0.2700\n",
      "Epoch 1 Batch 4700 Loss 2.7291 Accuracy 0.2711\n",
      "Epoch 1 Batch 4750 Loss 2.7196 Accuracy 0.2721\n",
      "Epoch 1 Batch 4800 Loss 2.7100 Accuracy 0.2731\n",
      "Epoch 1 Batch 4850 Loss 2.7004 Accuracy 0.2742\n",
      "Epoch 1 Batch 4900 Loss 2.6910 Accuracy 0.2752\n",
      "Epoch 1 Batch 4950 Loss 2.6825 Accuracy 0.2762\n",
      "Epoch 1 Batch 5000 Loss 2.6732 Accuracy 0.2772\n",
      "Epoch 1 Batch 5050 Loss 2.6643 Accuracy 0.2781\n",
      "Epoch 1 Batch 5100 Loss 2.6553 Accuracy 0.2791\n",
      "Epoch 1 Batch 5150 Loss 2.6466 Accuracy 0.2800\n",
      "Epoch 1 Batch 5200 Loss 2.6384 Accuracy 0.2809\n",
      "Epoch 1 Batch 5250 Loss 2.6297 Accuracy 0.2818\n",
      "Epoch 1 Batch 5300 Loss 2.6210 Accuracy 0.2827\n",
      "Epoch 1 Batch 5350 Loss 2.6125 Accuracy 0.2835\n",
      "Epoch 1 Batch 5400 Loss 2.6040 Accuracy 0.2844\n",
      "Epoch 1 Batch 5450 Loss 2.5958 Accuracy 0.2852\n",
      "Epoch 1 Batch 5500 Loss 2.5876 Accuracy 0.2861\n",
      "Epoch 1 Batch 5550 Loss 2.5795 Accuracy 0.2869\n",
      "Epoch 1 Batch 5600 Loss 2.5714 Accuracy 0.2877\n",
      "Epoch 1 Batch 5650 Loss 2.5633 Accuracy 0.2885\n",
      "Epoch 1 Batch 5700 Loss 2.5557 Accuracy 0.2893\n",
      "Saving checkpoint for epoch 1 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-1\n",
      "Time taken for 1 epoch: 6049.463179588318 secs\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Batch 0 Loss 1.7148 Accuracy 0.3980\n",
      "Epoch 2 Batch 50 Loss 1.6665 Accuracy 0.3878\n",
      "Epoch 2 Batch 100 Loss 1.6805 Accuracy 0.3882\n",
      "Epoch 2 Batch 150 Loss 1.6792 Accuracy 0.3883\n",
      "Epoch 2 Batch 200 Loss 1.6729 Accuracy 0.3902\n",
      "Epoch 2 Batch 250 Loss 1.6700 Accuracy 0.3903\n",
      "Epoch 2 Batch 300 Loss 1.6671 Accuracy 0.3902\n",
      "Epoch 2 Batch 350 Loss 1.6623 Accuracy 0.3902\n",
      "Epoch 2 Batch 400 Loss 1.6549 Accuracy 0.3905\n",
      "Epoch 2 Batch 450 Loss 1.6480 Accuracy 0.3913\n",
      "Epoch 2 Batch 500 Loss 1.6433 Accuracy 0.3914\n",
      "Epoch 2 Batch 550 Loss 1.6401 Accuracy 0.3921\n",
      "Epoch 2 Batch 600 Loss 1.6373 Accuracy 0.3923\n",
      "Epoch 2 Batch 650 Loss 1.6306 Accuracy 0.3931\n",
      "Epoch 2 Batch 700 Loss 1.6273 Accuracy 0.3936\n",
      "Epoch 2 Batch 750 Loss 1.6237 Accuracy 0.3945\n",
      "Epoch 2 Batch 800 Loss 1.6198 Accuracy 0.3947\n",
      "Epoch 2 Batch 850 Loss 1.6164 Accuracy 0.3951\n",
      "Epoch 2 Batch 900 Loss 1.6138 Accuracy 0.3954\n",
      "Epoch 2 Batch 950 Loss 1.6099 Accuracy 0.3957\n",
      "Epoch 2 Batch 1000 Loss 1.6059 Accuracy 0.3959\n",
      "Epoch 2 Batch 1050 Loss 1.6030 Accuracy 0.3963\n",
      "Epoch 2 Batch 1100 Loss 1.6005 Accuracy 0.3965\n",
      "Epoch 2 Batch 1150 Loss 1.5977 Accuracy 0.3969\n",
      "Epoch 2 Batch 1200 Loss 1.5935 Accuracy 0.3972\n",
      "Epoch 2 Batch 1250 Loss 1.5900 Accuracy 0.3980\n",
      "Epoch 2 Batch 1300 Loss 1.5863 Accuracy 0.3984\n",
      "Epoch 2 Batch 1350 Loss 1.5829 Accuracy 0.3991\n",
      "Epoch 2 Batch 1400 Loss 1.5786 Accuracy 0.4001\n",
      "Epoch 2 Batch 1450 Loss 1.5744 Accuracy 0.4010\n",
      "Epoch 2 Batch 1500 Loss 1.5702 Accuracy 0.4018\n",
      "Epoch 2 Batch 1550 Loss 1.5667 Accuracy 0.4027\n",
      "Epoch 2 Batch 1600 Loss 1.5626 Accuracy 0.4037\n",
      "Epoch 2 Batch 1650 Loss 1.5592 Accuracy 0.4046\n",
      "Epoch 2 Batch 1700 Loss 1.5551 Accuracy 0.4055\n",
      "Epoch 2 Batch 1750 Loss 1.5516 Accuracy 0.4064\n",
      "Epoch 2 Batch 1800 Loss 1.5481 Accuracy 0.4074\n",
      "Epoch 2 Batch 1850 Loss 1.5441 Accuracy 0.4084\n",
      "Epoch 2 Batch 1900 Loss 1.5404 Accuracy 0.4094\n",
      "Epoch 2 Batch 1950 Loss 1.5364 Accuracy 0.4103\n",
      "Epoch 2 Batch 2000 Loss 1.5324 Accuracy 0.4111\n",
      "Epoch 2 Batch 2050 Loss 1.5289 Accuracy 0.4119\n",
      "Epoch 2 Batch 2100 Loss 1.5254 Accuracy 0.4125\n",
      "Epoch 2 Batch 2150 Loss 1.5205 Accuracy 0.4131\n",
      "Epoch 2 Batch 2200 Loss 1.5149 Accuracy 0.4135\n",
      "Epoch 2 Batch 2250 Loss 1.5101 Accuracy 0.4139\n",
      "Epoch 2 Batch 2300 Loss 1.5060 Accuracy 0.4141\n",
      "Epoch 2 Batch 2350 Loss 1.5017 Accuracy 0.4147\n",
      "Epoch 2 Batch 2400 Loss 1.4967 Accuracy 0.4153\n",
      "Epoch 2 Batch 2450 Loss 1.4924 Accuracy 0.4159\n",
      "Epoch 2 Batch 2500 Loss 1.4877 Accuracy 0.4165\n",
      "Epoch 2 Batch 2550 Loss 1.4830 Accuracy 0.4171\n",
      "Epoch 2 Batch 2600 Loss 1.4784 Accuracy 0.4177\n",
      "Epoch 2 Batch 2650 Loss 1.4741 Accuracy 0.4183\n",
      "Epoch 2 Batch 2700 Loss 1.4693 Accuracy 0.4188\n",
      "Epoch 2 Batch 2750 Loss 1.4649 Accuracy 0.4194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 2800 Loss 1.4608 Accuracy 0.4200\n",
      "Epoch 2 Batch 2850 Loss 1.4563 Accuracy 0.4206\n",
      "Epoch 2 Batch 2900 Loss 1.4525 Accuracy 0.4212\n",
      "Epoch 2 Batch 2950 Loss 1.4487 Accuracy 0.4217\n",
      "Epoch 2 Batch 3000 Loss 1.4446 Accuracy 0.4222\n",
      "Epoch 2 Batch 3050 Loss 1.4412 Accuracy 0.4227\n",
      "Epoch 2 Batch 3100 Loss 1.4376 Accuracy 0.4232\n",
      "Epoch 2 Batch 3150 Loss 1.4337 Accuracy 0.4238\n",
      "Epoch 2 Batch 3200 Loss 1.4296 Accuracy 0.4243\n",
      "Epoch 2 Batch 3250 Loss 1.4258 Accuracy 0.4248\n",
      "Epoch 2 Batch 3300 Loss 1.4222 Accuracy 0.4253\n",
      "Epoch 2 Batch 3350 Loss 1.4184 Accuracy 0.4258\n",
      "Epoch 2 Batch 3400 Loss 1.4150 Accuracy 0.4263\n",
      "Epoch 2 Batch 3450 Loss 1.4115 Accuracy 0.4268\n",
      "Epoch 2 Batch 3500 Loss 1.4077 Accuracy 0.4274\n",
      "Epoch 2 Batch 3550 Loss 1.4041 Accuracy 0.4279\n",
      "Epoch 2 Batch 3600 Loss 1.4006 Accuracy 0.4284\n",
      "Epoch 2 Batch 3650 Loss 1.3976 Accuracy 0.4289\n",
      "Epoch 2 Batch 3700 Loss 1.3947 Accuracy 0.4295\n",
      "Epoch 2 Batch 3750 Loss 1.3917 Accuracy 0.4301\n",
      "Epoch 2 Batch 3800 Loss 1.3886 Accuracy 0.4307\n",
      "Epoch 2 Batch 3850 Loss 1.3858 Accuracy 0.4313\n",
      "Epoch 2 Batch 3900 Loss 1.3826 Accuracy 0.4318\n",
      "Epoch 2 Batch 3950 Loss 1.3796 Accuracy 0.4324\n",
      "Epoch 2 Batch 4000 Loss 1.3768 Accuracy 0.4329\n",
      "Epoch 2 Batch 4050 Loss 1.3740 Accuracy 0.4334\n",
      "Epoch 2 Batch 4100 Loss 1.3717 Accuracy 0.4339\n",
      "Epoch 2 Batch 4150 Loss 1.3700 Accuracy 0.4342\n",
      "Epoch 2 Batch 4200 Loss 1.3691 Accuracy 0.4344\n",
      "Epoch 2 Batch 4250 Loss 1.3682 Accuracy 0.4345\n",
      "Epoch 2 Batch 4300 Loss 1.3673 Accuracy 0.4346\n",
      "Epoch 2 Batch 4350 Loss 1.3669 Accuracy 0.4347\n",
      "Epoch 2 Batch 4400 Loss 1.3668 Accuracy 0.4347\n",
      "Epoch 2 Batch 4450 Loss 1.3665 Accuracy 0.4348\n",
      "Epoch 2 Batch 4500 Loss 1.3663 Accuracy 0.4348\n",
      "Epoch 2 Batch 4550 Loss 1.3664 Accuracy 0.4349\n",
      "Epoch 2 Batch 4600 Loss 1.3666 Accuracy 0.4348\n",
      "Epoch 2 Batch 4650 Loss 1.3667 Accuracy 0.4349\n",
      "Epoch 2 Batch 4700 Loss 1.3666 Accuracy 0.4349\n",
      "Epoch 2 Batch 4750 Loss 1.3666 Accuracy 0.4349\n",
      "Epoch 2 Batch 4800 Loss 1.3665 Accuracy 0.4350\n",
      "Epoch 2 Batch 4850 Loss 1.3663 Accuracy 0.4350\n",
      "Epoch 2 Batch 4900 Loss 1.3662 Accuracy 0.4350\n",
      "Epoch 2 Batch 4950 Loss 1.3662 Accuracy 0.4350\n",
      "Epoch 2 Batch 5000 Loss 1.3661 Accuracy 0.4351\n",
      "Epoch 2 Batch 5050 Loss 1.3661 Accuracy 0.4350\n",
      "Epoch 2 Batch 5100 Loss 1.3658 Accuracy 0.4350\n",
      "Epoch 2 Batch 5150 Loss 1.3661 Accuracy 0.4350\n",
      "Epoch 2 Batch 5200 Loss 1.3661 Accuracy 0.4350\n",
      "Epoch 2 Batch 5250 Loss 1.3662 Accuracy 0.4349\n",
      "Epoch 2 Batch 5300 Loss 1.3661 Accuracy 0.4348\n",
      "Epoch 2 Batch 5350 Loss 1.3658 Accuracy 0.4346\n",
      "Epoch 2 Batch 5400 Loss 1.3657 Accuracy 0.4346\n",
      "Epoch 2 Batch 5450 Loss 1.3654 Accuracy 0.4345\n",
      "Epoch 2 Batch 5500 Loss 1.3650 Accuracy 0.4344\n",
      "Epoch 2 Batch 5550 Loss 1.3648 Accuracy 0.4344\n",
      "Epoch 2 Batch 5600 Loss 1.3646 Accuracy 0.4344\n",
      "Epoch 2 Batch 5650 Loss 1.3642 Accuracy 0.4343\n",
      "Epoch 2 Batch 5700 Loss 1.3638 Accuracy 0.4343\n",
      "Saving checkpoint for epoch 2 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-2\n",
      "Time taken for 1 epoch: 7532.937515497208 secs\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.4192 Accuracy 0.3980\n",
      "Epoch 3 Batch 50 Loss 1.3577 Accuracy 0.4361\n",
      "Epoch 3 Batch 100 Loss 1.3476 Accuracy 0.4364\n",
      "Epoch 3 Batch 150 Loss 1.3425 Accuracy 0.4365\n",
      "Epoch 3 Batch 200 Loss 1.3420 Accuracy 0.4365\n",
      "Epoch 3 Batch 250 Loss 1.3377 Accuracy 0.4370\n",
      "Epoch 3 Batch 300 Loss 1.3338 Accuracy 0.4378\n",
      "Epoch 3 Batch 350 Loss 1.3290 Accuracy 0.4379\n",
      "Epoch 3 Batch 400 Loss 1.3215 Accuracy 0.4381\n",
      "Epoch 3 Batch 450 Loss 1.3183 Accuracy 0.4380\n",
      "Epoch 3 Batch 500 Loss 1.3144 Accuracy 0.4377\n",
      "Epoch 3 Batch 550 Loss 1.3121 Accuracy 0.4379\n",
      "Epoch 3 Batch 600 Loss 1.3098 Accuracy 0.4379\n",
      "Epoch 3 Batch 650 Loss 1.3089 Accuracy 0.4383\n",
      "Epoch 3 Batch 700 Loss 1.3096 Accuracy 0.4384\n",
      "Epoch 3 Batch 750 Loss 1.3109 Accuracy 0.4391\n",
      "Epoch 3 Batch 800 Loss 1.3088 Accuracy 0.4397\n",
      "Epoch 3 Batch 850 Loss 1.3062 Accuracy 0.4399\n",
      "Epoch 3 Batch 900 Loss 1.3069 Accuracy 0.4403\n",
      "Epoch 3 Batch 950 Loss 1.3039 Accuracy 0.4403\n",
      "Epoch 3 Batch 1000 Loss 1.3005 Accuracy 0.4404\n",
      "Epoch 3 Batch 1050 Loss 1.2991 Accuracy 0.4406\n",
      "Epoch 3 Batch 1100 Loss 1.2972 Accuracy 0.4410\n",
      "Epoch 3 Batch 1150 Loss 1.2957 Accuracy 0.4411\n",
      "Epoch 3 Batch 1200 Loss 1.2935 Accuracy 0.4414\n",
      "Epoch 3 Batch 1250 Loss 1.2911 Accuracy 0.4417\n",
      "Epoch 3 Batch 1300 Loss 1.2897 Accuracy 0.4420\n",
      "Epoch 3 Batch 1350 Loss 1.2875 Accuracy 0.4427\n",
      "Epoch 3 Batch 1400 Loss 1.2847 Accuracy 0.4434\n",
      "Epoch 3 Batch 1450 Loss 1.2821 Accuracy 0.4441\n",
      "Epoch 3 Batch 1500 Loss 1.2785 Accuracy 0.4448\n",
      "Epoch 3 Batch 1550 Loss 1.2753 Accuracy 0.4457\n",
      "Epoch 3 Batch 1600 Loss 1.2731 Accuracy 0.4465\n",
      "Epoch 3 Batch 1650 Loss 1.2709 Accuracy 0.4472\n",
      "Epoch 3 Batch 1700 Loss 1.2675 Accuracy 0.4482\n",
      "Epoch 3 Batch 1750 Loss 1.2648 Accuracy 0.4491\n",
      "Epoch 3 Batch 1800 Loss 1.2622 Accuracy 0.4501\n",
      "Epoch 3 Batch 1850 Loss 1.2596 Accuracy 0.4511\n",
      "Epoch 3 Batch 1900 Loss 1.2567 Accuracy 0.4519\n",
      "Epoch 3 Batch 1950 Loss 1.2549 Accuracy 0.4526\n",
      "Epoch 3 Batch 2000 Loss 1.2521 Accuracy 0.4532\n",
      "Epoch 3 Batch 2050 Loss 1.2495 Accuracy 0.4537\n",
      "Epoch 3 Batch 2100 Loss 1.2465 Accuracy 0.4541\n",
      "Epoch 3 Batch 2150 Loss 1.2434 Accuracy 0.4544\n",
      "Epoch 3 Batch 2200 Loss 1.2396 Accuracy 0.4547\n",
      "Epoch 3 Batch 2250 Loss 1.2364 Accuracy 0.4549\n",
      "Epoch 3 Batch 2300 Loss 1.2328 Accuracy 0.4551\n",
      "Epoch 3 Batch 2350 Loss 1.2300 Accuracy 0.4554\n",
      "Epoch 3 Batch 2400 Loss 1.2265 Accuracy 0.4557\n",
      "Epoch 3 Batch 2450 Loss 1.2237 Accuracy 0.4559\n",
      "Epoch 3 Batch 2500 Loss 1.2206 Accuracy 0.4563\n",
      "Epoch 3 Batch 2550 Loss 1.2176 Accuracy 0.4567\n",
      "Epoch 3 Batch 2600 Loss 1.2145 Accuracy 0.4571\n",
      "Epoch 3 Batch 2650 Loss 1.2111 Accuracy 0.4576\n",
      "Epoch 3 Batch 2700 Loss 1.2074 Accuracy 0.4580\n",
      "Epoch 3 Batch 2750 Loss 1.2047 Accuracy 0.4583\n",
      "Epoch 3 Batch 2800 Loss 1.2023 Accuracy 0.4587\n",
      "Epoch 3 Batch 2850 Loss 1.1996 Accuracy 0.4592\n",
      "Epoch 3 Batch 2900 Loss 1.1973 Accuracy 0.4596\n",
      "Epoch 3 Batch 2950 Loss 1.1945 Accuracy 0.4600\n",
      "Epoch 3 Batch 3000 Loss 1.1919 Accuracy 0.4604\n",
      "Epoch 3 Batch 3050 Loss 1.1895 Accuracy 0.4608\n",
      "Epoch 3 Batch 3100 Loss 1.1870 Accuracy 0.4611\n",
      "Epoch 3 Batch 3150 Loss 1.1846 Accuracy 0.4615\n",
      "Epoch 3 Batch 3200 Loss 1.1816 Accuracy 0.4618\n",
      "Epoch 3 Batch 3250 Loss 1.1787 Accuracy 0.4621\n",
      "Epoch 3 Batch 3300 Loss 1.1761 Accuracy 0.4624\n",
      "Epoch 3 Batch 3350 Loss 1.1733 Accuracy 0.4628\n",
      "Epoch 3 Batch 3400 Loss 1.1708 Accuracy 0.4632\n",
      "Epoch 3 Batch 3450 Loss 1.1685 Accuracy 0.4635\n",
      "Epoch 3 Batch 3500 Loss 1.1663 Accuracy 0.4640\n",
      "Epoch 3 Batch 3550 Loss 1.1638 Accuracy 0.4643\n",
      "Epoch 3 Batch 3600 Loss 1.1616 Accuracy 0.4647\n",
      "Epoch 3 Batch 3650 Loss 1.1599 Accuracy 0.4650\n",
      "Epoch 3 Batch 3700 Loss 1.1579 Accuracy 0.4654\n",
      "Epoch 3 Batch 3750 Loss 1.1556 Accuracy 0.4659\n",
      "Epoch 3 Batch 3800 Loss 1.1535 Accuracy 0.4662\n",
      "Epoch 3 Batch 3850 Loss 1.1514 Accuracy 0.4666\n",
      "Epoch 3 Batch 3900 Loss 1.1497 Accuracy 0.4670\n",
      "Epoch 3 Batch 3950 Loss 1.1478 Accuracy 0.4674\n",
      "Epoch 3 Batch 4000 Loss 1.1457 Accuracy 0.4678\n",
      "Epoch 3 Batch 4050 Loss 1.1441 Accuracy 0.4682\n",
      "Epoch 3 Batch 4100 Loss 1.1429 Accuracy 0.4685\n",
      "Epoch 3 Batch 4150 Loss 1.1421 Accuracy 0.4687\n",
      "Epoch 3 Batch 4200 Loss 1.1421 Accuracy 0.4687\n",
      "Epoch 3 Batch 4250 Loss 1.1420 Accuracy 0.4687\n",
      "Epoch 3 Batch 4300 Loss 1.1425 Accuracy 0.4687\n",
      "Epoch 3 Batch 4350 Loss 1.1432 Accuracy 0.4686\n",
      "Epoch 3 Batch 4400 Loss 1.1436 Accuracy 0.4686\n",
      "Epoch 3 Batch 4450 Loss 1.1444 Accuracy 0.4685\n",
      "Epoch 3 Batch 4500 Loss 1.1453 Accuracy 0.4684\n",
      "Epoch 3 Batch 4550 Loss 1.1461 Accuracy 0.4683\n",
      "Epoch 3 Batch 4600 Loss 1.1471 Accuracy 0.4681\n",
      "Epoch 3 Batch 4650 Loss 1.1480 Accuracy 0.4681\n",
      "Epoch 3 Batch 4700 Loss 1.1489 Accuracy 0.4679\n",
      "Epoch 3 Batch 4750 Loss 1.1496 Accuracy 0.4678\n",
      "Epoch 3 Batch 4800 Loss 1.1504 Accuracy 0.4677\n",
      "Epoch 3 Batch 4850 Loss 1.1514 Accuracy 0.4676\n",
      "Epoch 3 Batch 4900 Loss 1.1522 Accuracy 0.4675\n",
      "Epoch 3 Batch 4950 Loss 1.1529 Accuracy 0.4673\n",
      "Epoch 3 Batch 5000 Loss 1.1541 Accuracy 0.4671\n",
      "Epoch 3 Batch 5050 Loss 1.1549 Accuracy 0.4670\n",
      "Epoch 3 Batch 5100 Loss 1.1558 Accuracy 0.4668\n",
      "Epoch 3 Batch 5150 Loss 1.1568 Accuracy 0.4667\n",
      "Epoch 3 Batch 5200 Loss 1.1575 Accuracy 0.4665\n",
      "Epoch 3 Batch 5250 Loss 1.1584 Accuracy 0.4663\n",
      "Epoch 3 Batch 5300 Loss 1.1590 Accuracy 0.4660\n",
      "Epoch 3 Batch 5350 Loss 1.1598 Accuracy 0.4659\n",
      "Epoch 3 Batch 5400 Loss 1.1606 Accuracy 0.4656\n",
      "Epoch 3 Batch 5450 Loss 1.1611 Accuracy 0.4655\n",
      "Epoch 3 Batch 5500 Loss 1.1617 Accuracy 0.4653\n",
      "Epoch 3 Batch 5550 Loss 1.1623 Accuracy 0.4651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 5600 Loss 1.1628 Accuracy 0.4649\n",
      "Epoch 3 Batch 5650 Loss 1.1632 Accuracy 0.4648\n",
      "Epoch 3 Batch 5700 Loss 1.1637 Accuracy 0.4646\n",
      "Saving checkpoint for epoch 3 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-3\n",
      "Time taken for 1 epoch: 6577.199537038803 secs\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.2336 Accuracy 0.4342\n",
      "Epoch 4 Batch 50 Loss 1.1992 Accuracy 0.4528\n",
      "Epoch 4 Batch 100 Loss 1.2025 Accuracy 0.4542\n",
      "Epoch 4 Batch 150 Loss 1.2144 Accuracy 0.4547\n",
      "Epoch 4 Batch 200 Loss 1.2165 Accuracy 0.4549\n",
      "Epoch 4 Batch 250 Loss 1.2159 Accuracy 0.4557\n",
      "Epoch 4 Batch 300 Loss 1.2160 Accuracy 0.4560\n",
      "Epoch 4 Batch 350 Loss 1.2169 Accuracy 0.4556\n",
      "Epoch 4 Batch 400 Loss 1.2139 Accuracy 0.4562\n",
      "Epoch 4 Batch 450 Loss 1.2096 Accuracy 0.4563\n",
      "Epoch 4 Batch 500 Loss 1.2062 Accuracy 0.4566\n",
      "Epoch 4 Batch 550 Loss 1.2046 Accuracy 0.4567\n",
      "Epoch 4 Batch 600 Loss 1.2028 Accuracy 0.4566\n",
      "Epoch 4 Batch 650 Loss 1.2029 Accuracy 0.4565\n",
      "Epoch 4 Batch 700 Loss 1.2016 Accuracy 0.4571\n",
      "Epoch 4 Batch 750 Loss 1.2002 Accuracy 0.4573\n",
      "Epoch 4 Batch 800 Loss 1.1994 Accuracy 0.4576\n",
      "Epoch 4 Batch 850 Loss 1.1972 Accuracy 0.4580\n",
      "Epoch 4 Batch 900 Loss 1.1965 Accuracy 0.4578\n",
      "Epoch 4 Batch 950 Loss 1.1948 Accuracy 0.4577\n",
      "Epoch 4 Batch 1000 Loss 1.1936 Accuracy 0.4580\n",
      "Epoch 4 Batch 1050 Loss 1.1933 Accuracy 0.4583\n",
      "Epoch 4 Batch 1100 Loss 1.1910 Accuracy 0.4582\n",
      "Epoch 4 Batch 1150 Loss 1.1888 Accuracy 0.4584\n",
      "Epoch 4 Batch 1200 Loss 1.1867 Accuracy 0.4586\n",
      "Epoch 4 Batch 1250 Loss 1.1838 Accuracy 0.4589\n",
      "Epoch 4 Batch 1300 Loss 1.1815 Accuracy 0.4591\n",
      "Epoch 4 Batch 1350 Loss 1.1794 Accuracy 0.4598\n",
      "Epoch 4 Batch 1400 Loss 1.1763 Accuracy 0.4606\n",
      "Epoch 4 Batch 1450 Loss 1.1746 Accuracy 0.4613\n",
      "Epoch 4 Batch 1500 Loss 1.1719 Accuracy 0.4622\n",
      "Epoch 4 Batch 1550 Loss 1.1699 Accuracy 0.4630\n",
      "Epoch 4 Batch 1600 Loss 1.1670 Accuracy 0.4638\n",
      "Epoch 4 Batch 1650 Loss 1.1645 Accuracy 0.4646\n",
      "Epoch 4 Batch 1700 Loss 1.1621 Accuracy 0.4655\n",
      "Epoch 4 Batch 1750 Loss 1.1598 Accuracy 0.4663\n",
      "Epoch 4 Batch 1800 Loss 1.1569 Accuracy 0.4672\n",
      "Epoch 4 Batch 1850 Loss 1.1543 Accuracy 0.4679\n",
      "Epoch 4 Batch 1900 Loss 1.1512 Accuracy 0.4686\n",
      "Epoch 4 Batch 1950 Loss 1.1496 Accuracy 0.4694\n",
      "Epoch 4 Batch 2000 Loss 1.1473 Accuracy 0.4700\n",
      "Epoch 4 Batch 2050 Loss 1.1447 Accuracy 0.4705\n",
      "Epoch 4 Batch 2100 Loss 1.1419 Accuracy 0.4708\n",
      "Epoch 4 Batch 2150 Loss 1.1393 Accuracy 0.4710\n",
      "Epoch 4 Batch 2200 Loss 1.1358 Accuracy 0.4713\n",
      "Epoch 4 Batch 2250 Loss 1.1323 Accuracy 0.4714\n",
      "Epoch 4 Batch 2300 Loss 1.1292 Accuracy 0.4717\n",
      "Epoch 4 Batch 2350 Loss 1.1262 Accuracy 0.4720\n",
      "Epoch 4 Batch 2400 Loss 1.1233 Accuracy 0.4723\n",
      "Epoch 4 Batch 2450 Loss 1.1199 Accuracy 0.4726\n",
      "Epoch 4 Batch 2500 Loss 1.1171 Accuracy 0.4729\n",
      "Epoch 4 Batch 2550 Loss 1.1142 Accuracy 0.4732\n",
      "Epoch 4 Batch 2600 Loss 1.1114 Accuracy 0.4736\n",
      "Epoch 4 Batch 2650 Loss 1.1082 Accuracy 0.4739\n",
      "Epoch 4 Batch 2700 Loss 1.1054 Accuracy 0.4743\n",
      "Epoch 4 Batch 2750 Loss 1.1033 Accuracy 0.4746\n",
      "Epoch 4 Batch 2800 Loss 1.1008 Accuracy 0.4749\n",
      "Epoch 4 Batch 2850 Loss 1.0983 Accuracy 0.4753\n",
      "Epoch 4 Batch 2900 Loss 1.0953 Accuracy 0.4757\n",
      "Epoch 4 Batch 2950 Loss 1.0934 Accuracy 0.4760\n",
      "Epoch 4 Batch 3000 Loss 1.0910 Accuracy 0.4763\n",
      "Epoch 4 Batch 3050 Loss 1.0889 Accuracy 0.4766\n",
      "Epoch 4 Batch 3100 Loss 1.0870 Accuracy 0.4769\n",
      "Epoch 4 Batch 3150 Loss 1.0846 Accuracy 0.4772\n",
      "Epoch 4 Batch 3200 Loss 1.0826 Accuracy 0.4775\n",
      "Epoch 4 Batch 3250 Loss 1.0806 Accuracy 0.4777\n",
      "Epoch 4 Batch 3300 Loss 1.0788 Accuracy 0.4780\n",
      "Epoch 4 Batch 3350 Loss 1.0768 Accuracy 0.4783\n",
      "Epoch 4 Batch 3400 Loss 1.0747 Accuracy 0.4786\n",
      "Epoch 4 Batch 3450 Loss 1.0727 Accuracy 0.4789\n",
      "Epoch 4 Batch 3500 Loss 1.0709 Accuracy 0.4792\n",
      "Epoch 4 Batch 3550 Loss 1.0686 Accuracy 0.4795\n",
      "Epoch 4 Batch 3600 Loss 1.0662 Accuracy 0.4798\n",
      "Epoch 4 Batch 3650 Loss 1.0640 Accuracy 0.4802\n",
      "Epoch 4 Batch 3700 Loss 1.0622 Accuracy 0.4806\n",
      "Epoch 4 Batch 3750 Loss 1.0602 Accuracy 0.4811\n",
      "Epoch 4 Batch 3800 Loss 1.0585 Accuracy 0.4814\n",
      "Epoch 4 Batch 3850 Loss 1.0569 Accuracy 0.4817\n",
      "Epoch 4 Batch 3900 Loss 1.0552 Accuracy 0.4821\n",
      "Epoch 4 Batch 3950 Loss 1.0537 Accuracy 0.4824\n",
      "Epoch 4 Batch 4000 Loss 1.0522 Accuracy 0.4828\n",
      "Epoch 4 Batch 4050 Loss 1.0506 Accuracy 0.4831\n",
      "Epoch 4 Batch 4100 Loss 1.0495 Accuracy 0.4833\n",
      "Epoch 4 Batch 4150 Loss 1.0486 Accuracy 0.4835\n",
      "Epoch 4 Batch 4200 Loss 1.0488 Accuracy 0.4835\n",
      "Epoch 4 Batch 4250 Loss 1.0493 Accuracy 0.4835\n",
      "Epoch 4 Batch 4300 Loss 1.0499 Accuracy 0.4835\n",
      "Epoch 4 Batch 4350 Loss 1.0508 Accuracy 0.4834\n",
      "Epoch 4 Batch 4400 Loss 1.0514 Accuracy 0.4833\n",
      "Epoch 4 Batch 4450 Loss 1.0527 Accuracy 0.4831\n",
      "Epoch 4 Batch 4500 Loss 1.0539 Accuracy 0.4830\n",
      "Epoch 4 Batch 4550 Loss 1.0551 Accuracy 0.4828\n",
      "Epoch 4 Batch 4600 Loss 1.0562 Accuracy 0.4826\n",
      "Epoch 4 Batch 4650 Loss 1.0574 Accuracy 0.4824\n",
      "Epoch 4 Batch 4700 Loss 1.0587 Accuracy 0.4823\n",
      "Epoch 4 Batch 4750 Loss 1.0599 Accuracy 0.4821\n",
      "Epoch 4 Batch 4800 Loss 1.0608 Accuracy 0.4820\n",
      "Epoch 4 Batch 4850 Loss 1.0618 Accuracy 0.4819\n",
      "Epoch 4 Batch 4900 Loss 1.0631 Accuracy 0.4817\n",
      "Epoch 4 Batch 4950 Loss 1.0644 Accuracy 0.4816\n",
      "Epoch 4 Batch 5000 Loss 1.0654 Accuracy 0.4814\n",
      "Epoch 4 Batch 5050 Loss 1.0665 Accuracy 0.4812\n",
      "Epoch 4 Batch 5100 Loss 1.0676 Accuracy 0.4810\n",
      "Epoch 4 Batch 5150 Loss 1.0688 Accuracy 0.4807\n",
      "Epoch 4 Batch 5200 Loss 1.0702 Accuracy 0.4805\n",
      "Epoch 4 Batch 5250 Loss 1.0711 Accuracy 0.4802\n",
      "Epoch 4 Batch 5300 Loss 1.0720 Accuracy 0.4800\n",
      "Epoch 4 Batch 5350 Loss 1.0728 Accuracy 0.4798\n",
      "Epoch 4 Batch 5400 Loss 1.0737 Accuracy 0.4795\n",
      "Epoch 4 Batch 5450 Loss 1.0745 Accuracy 0.4793\n",
      "Epoch 4 Batch 5500 Loss 1.0753 Accuracy 0.4790\n",
      "Epoch 4 Batch 5550 Loss 1.0761 Accuracy 0.4788\n",
      "Epoch 4 Batch 5600 Loss 1.0769 Accuracy 0.4786\n",
      "Epoch 4 Batch 5650 Loss 1.0774 Accuracy 0.4785\n",
      "Epoch 4 Batch 5700 Loss 1.0782 Accuracy 0.4783\n",
      "Saving checkpoint for epoch 4 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-4\n",
      "Time taken for 1 epoch: 7239.462972640991 secs\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 Batch 0 Loss 1.3189 Accuracy 0.4424\n",
      "Epoch 5 Batch 50 Loss 1.1671 Accuracy 0.4626\n",
      "Epoch 5 Batch 100 Loss 1.1553 Accuracy 0.4640\n",
      "Epoch 5 Batch 150 Loss 1.1625 Accuracy 0.4645\n",
      "Epoch 5 Batch 200 Loss 1.1547 Accuracy 0.4646\n",
      "Epoch 5 Batch 250 Loss 1.1512 Accuracy 0.4651\n",
      "Epoch 5 Batch 300 Loss 1.1511 Accuracy 0.4661\n",
      "Epoch 5 Batch 350 Loss 1.1489 Accuracy 0.4655\n",
      "Epoch 5 Batch 400 Loss 1.1467 Accuracy 0.4661\n",
      "Epoch 5 Batch 450 Loss 1.1459 Accuracy 0.4657\n",
      "Epoch 5 Batch 500 Loss 1.1439 Accuracy 0.4656\n",
      "Epoch 5 Batch 550 Loss 1.1427 Accuracy 0.4655\n",
      "Epoch 5 Batch 600 Loss 1.1433 Accuracy 0.4657\n",
      "Epoch 5 Batch 650 Loss 1.1412 Accuracy 0.4659\n",
      "Epoch 5 Batch 700 Loss 1.1404 Accuracy 0.4665\n",
      "Epoch 5 Batch 750 Loss 1.1395 Accuracy 0.4667\n",
      "Epoch 5 Batch 800 Loss 1.1382 Accuracy 0.4667\n",
      "Epoch 5 Batch 850 Loss 1.1374 Accuracy 0.4670\n",
      "Epoch 5 Batch 900 Loss 1.1349 Accuracy 0.4673\n",
      "Epoch 5 Batch 950 Loss 1.1324 Accuracy 0.4674\n",
      "Epoch 5 Batch 1000 Loss 1.1294 Accuracy 0.4676\n",
      "Epoch 5 Batch 1050 Loss 1.1275 Accuracy 0.4678\n",
      "Epoch 5 Batch 1100 Loss 1.1262 Accuracy 0.4680\n",
      "Epoch 5 Batch 1150 Loss 1.1255 Accuracy 0.4680\n",
      "Epoch 5 Batch 1200 Loss 1.1238 Accuracy 0.4682\n",
      "Epoch 5 Batch 1250 Loss 1.1219 Accuracy 0.4687\n",
      "Epoch 5 Batch 1300 Loss 1.1196 Accuracy 0.4692\n",
      "Epoch 5 Batch 1350 Loss 1.1176 Accuracy 0.4698\n",
      "Epoch 5 Batch 1400 Loss 1.1157 Accuracy 0.4702\n",
      "Epoch 5 Batch 1450 Loss 1.1132 Accuracy 0.4710\n",
      "Epoch 5 Batch 1500 Loss 1.1105 Accuracy 0.4718\n",
      "Epoch 5 Batch 1550 Loss 1.1076 Accuracy 0.4725\n",
      "Epoch 5 Batch 1600 Loss 1.1043 Accuracy 0.4732\n",
      "Epoch 5 Batch 1650 Loss 1.1023 Accuracy 0.4739\n",
      "Epoch 5 Batch 1700 Loss 1.1002 Accuracy 0.4748\n",
      "Epoch 5 Batch 1750 Loss 1.0982 Accuracy 0.4756\n",
      "Epoch 5 Batch 1800 Loss 1.0963 Accuracy 0.4764\n",
      "Epoch 5 Batch 1850 Loss 1.0941 Accuracy 0.4771\n",
      "Epoch 5 Batch 1900 Loss 1.0921 Accuracy 0.4778\n",
      "Epoch 5 Batch 1950 Loss 1.0899 Accuracy 0.4786\n",
      "Epoch 5 Batch 2000 Loss 1.0883 Accuracy 0.4793\n",
      "Epoch 5 Batch 2050 Loss 1.0862 Accuracy 0.4797\n",
      "Epoch 5 Batch 2100 Loss 1.0834 Accuracy 0.4802\n",
      "Epoch 5 Batch 2150 Loss 1.0807 Accuracy 0.4804\n",
      "Epoch 5 Batch 2200 Loss 1.0778 Accuracy 0.4806\n",
      "Epoch 5 Batch 2250 Loss 1.0748 Accuracy 0.4810\n",
      "Epoch 5 Batch 2300 Loss 1.0714 Accuracy 0.4813\n",
      "Epoch 5 Batch 2350 Loss 1.0683 Accuracy 0.4816\n",
      "Epoch 5 Batch 2400 Loss 1.0659 Accuracy 0.4818\n",
      "Epoch 5 Batch 2450 Loss 1.0631 Accuracy 0.4821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 2500 Loss 1.0603 Accuracy 0.4825\n",
      "Epoch 5 Batch 2550 Loss 1.0573 Accuracy 0.4827\n",
      "Epoch 5 Batch 2600 Loss 1.0544 Accuracy 0.4830\n",
      "Epoch 5 Batch 2650 Loss 1.0516 Accuracy 0.4833\n",
      "Epoch 5 Batch 2700 Loss 1.0488 Accuracy 0.4836\n",
      "Epoch 5 Batch 2750 Loss 1.0462 Accuracy 0.4840\n",
      "Epoch 5 Batch 2800 Loss 1.0437 Accuracy 0.4843\n",
      "Epoch 5 Batch 2850 Loss 1.0418 Accuracy 0.4846\n",
      "Epoch 5 Batch 2900 Loss 1.0395 Accuracy 0.4849\n",
      "Epoch 5 Batch 2950 Loss 1.0373 Accuracy 0.4852\n",
      "Epoch 5 Batch 3000 Loss 1.0353 Accuracy 0.4856\n",
      "Epoch 5 Batch 3050 Loss 1.0332 Accuracy 0.4858\n",
      "Epoch 5 Batch 3100 Loss 1.0308 Accuracy 0.4860\n",
      "Epoch 5 Batch 3150 Loss 1.0288 Accuracy 0.4863\n",
      "Epoch 5 Batch 3200 Loss 1.0269 Accuracy 0.4865\n",
      "Epoch 5 Batch 3250 Loss 1.0252 Accuracy 0.4867\n",
      "Epoch 5 Batch 3300 Loss 1.0233 Accuracy 0.4870\n",
      "Epoch 5 Batch 3350 Loss 1.0214 Accuracy 0.4873\n",
      "Epoch 5 Batch 3400 Loss 1.0191 Accuracy 0.4877\n",
      "Epoch 5 Batch 3450 Loss 1.0172 Accuracy 0.4880\n",
      "Epoch 5 Batch 3500 Loss 1.0154 Accuracy 0.4883\n",
      "Epoch 5 Batch 3550 Loss 1.0133 Accuracy 0.4886\n",
      "Epoch 5 Batch 3600 Loss 1.0114 Accuracy 0.4889\n",
      "Epoch 5 Batch 3650 Loss 1.0096 Accuracy 0.4893\n",
      "Epoch 5 Batch 3700 Loss 1.0077 Accuracy 0.4897\n",
      "Epoch 5 Batch 3750 Loss 1.0059 Accuracy 0.4900\n",
      "Epoch 5 Batch 3800 Loss 1.0042 Accuracy 0.4904\n",
      "Epoch 5 Batch 3850 Loss 1.0026 Accuracy 0.4907\n",
      "Epoch 5 Batch 3900 Loss 1.0011 Accuracy 0.4911\n",
      "Epoch 5 Batch 3950 Loss 0.9997 Accuracy 0.4913\n",
      "Epoch 5 Batch 4000 Loss 0.9984 Accuracy 0.4917\n",
      "Epoch 5 Batch 4050 Loss 0.9970 Accuracy 0.4920\n",
      "Epoch 5 Batch 4100 Loss 0.9957 Accuracy 0.4922\n",
      "Epoch 5 Batch 4150 Loss 0.9956 Accuracy 0.4922\n",
      "Epoch 5 Batch 4200 Loss 0.9957 Accuracy 0.4923\n",
      "Epoch 5 Batch 4250 Loss 0.9959 Accuracy 0.4923\n",
      "Epoch 5 Batch 4300 Loss 0.9964 Accuracy 0.4923\n",
      "Epoch 5 Batch 4350 Loss 0.9971 Accuracy 0.4922\n",
      "Epoch 5 Batch 4400 Loss 0.9980 Accuracy 0.4920\n",
      "Epoch 5 Batch 4450 Loss 0.9992 Accuracy 0.4919\n",
      "Epoch 5 Batch 4500 Loss 1.0006 Accuracy 0.4918\n",
      "Epoch 5 Batch 4550 Loss 1.0018 Accuracy 0.4916\n",
      "Epoch 5 Batch 4600 Loss 1.0031 Accuracy 0.4914\n",
      "Epoch 5 Batch 4650 Loss 1.0046 Accuracy 0.4912\n",
      "Epoch 5 Batch 4700 Loss 1.0059 Accuracy 0.4910\n",
      "Epoch 5 Batch 4750 Loss 1.0071 Accuracy 0.4908\n",
      "Epoch 5 Batch 4800 Loss 1.0082 Accuracy 0.4906\n",
      "Epoch 5 Batch 4850 Loss 1.0091 Accuracy 0.4905\n",
      "Epoch 5 Batch 4900 Loss 1.0103 Accuracy 0.4904\n",
      "Epoch 5 Batch 4950 Loss 1.0112 Accuracy 0.4902\n",
      "Epoch 5 Batch 5000 Loss 1.0124 Accuracy 0.4900\n",
      "Epoch 5 Batch 5050 Loss 1.0137 Accuracy 0.4898\n",
      "Epoch 5 Batch 5100 Loss 1.0153 Accuracy 0.4896\n",
      "Epoch 5 Batch 5150 Loss 1.0165 Accuracy 0.4894\n",
      "Epoch 5 Batch 5200 Loss 1.0178 Accuracy 0.4891\n",
      "Epoch 5 Batch 5250 Loss 1.0188 Accuracy 0.4888\n",
      "Epoch 5 Batch 5300 Loss 1.0199 Accuracy 0.4885\n",
      "Epoch 5 Batch 5350 Loss 1.0207 Accuracy 0.4883\n",
      "Epoch 5 Batch 5400 Loss 1.0216 Accuracy 0.4880\n",
      "Epoch 5 Batch 5450 Loss 1.0225 Accuracy 0.4878\n",
      "Epoch 5 Batch 5500 Loss 1.0234 Accuracy 0.4875\n",
      "Epoch 5 Batch 5550 Loss 1.0242 Accuracy 0.4873\n",
      "Epoch 5 Batch 5600 Loss 1.0249 Accuracy 0.4871\n",
      "Epoch 5 Batch 5650 Loss 1.0259 Accuracy 0.4868\n",
      "Epoch 5 Batch 5700 Loss 1.0268 Accuracy 0.4867\n",
      "Saving checkpoint for epoch 5 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-5\n",
      "Time taken for 1 epoch: 7694.68957901001 secs\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 Batch 0 Loss 0.9418 Accuracy 0.4712\n",
      "Epoch 6 Batch 50 Loss 1.1155 Accuracy 0.4728\n",
      "Epoch 6 Batch 100 Loss 1.1129 Accuracy 0.4714\n",
      "Epoch 6 Batch 150 Loss 1.1103 Accuracy 0.4711\n",
      "Epoch 6 Batch 200 Loss 1.1147 Accuracy 0.4725\n",
      "Epoch 6 Batch 250 Loss 1.1144 Accuracy 0.4731\n",
      "Epoch 6 Batch 300 Loss 1.1109 Accuracy 0.4735\n",
      "Epoch 6 Batch 350 Loss 1.1105 Accuracy 0.4725\n",
      "Epoch 6 Batch 400 Loss 1.1087 Accuracy 0.4723\n",
      "Epoch 6 Batch 450 Loss 1.1046 Accuracy 0.4721\n",
      "Epoch 6 Batch 500 Loss 1.1018 Accuracy 0.4726\n",
      "Epoch 6 Batch 550 Loss 1.0995 Accuracy 0.4723\n",
      "Epoch 6 Batch 600 Loss 1.0986 Accuracy 0.4725\n",
      "Epoch 6 Batch 650 Loss 1.0987 Accuracy 0.4725\n",
      "Epoch 6 Batch 700 Loss 1.0984 Accuracy 0.4729\n",
      "Epoch 6 Batch 750 Loss 1.0974 Accuracy 0.4731\n",
      "Epoch 6 Batch 800 Loss 1.0966 Accuracy 0.4735\n",
      "Epoch 6 Batch 850 Loss 1.0959 Accuracy 0.4737\n",
      "Epoch 6 Batch 900 Loss 1.0940 Accuracy 0.4737\n",
      "Epoch 6 Batch 950 Loss 1.0922 Accuracy 0.4736\n",
      "Epoch 6 Batch 1000 Loss 1.0897 Accuracy 0.4737\n",
      "Epoch 6 Batch 1050 Loss 1.0887 Accuracy 0.4738\n",
      "Epoch 6 Batch 1100 Loss 1.0877 Accuracy 0.4740\n",
      "Epoch 6 Batch 1150 Loss 1.0857 Accuracy 0.4743\n",
      "Epoch 6 Batch 1200 Loss 1.0838 Accuracy 0.4747\n",
      "Epoch 6 Batch 1250 Loss 1.0821 Accuracy 0.4749\n",
      "Epoch 6 Batch 1300 Loss 1.0803 Accuracy 0.4751\n",
      "Epoch 6 Batch 1350 Loss 1.0779 Accuracy 0.4759\n",
      "Epoch 6 Batch 1400 Loss 1.0758 Accuracy 0.4766\n",
      "Epoch 6 Batch 1450 Loss 1.0727 Accuracy 0.4775\n",
      "Epoch 6 Batch 1500 Loss 1.0703 Accuracy 0.4782\n",
      "Epoch 6 Batch 1550 Loss 1.0677 Accuracy 0.4792\n",
      "Epoch 6 Batch 1600 Loss 1.0650 Accuracy 0.4798\n",
      "Epoch 6 Batch 1650 Loss 1.0630 Accuracy 0.4807\n",
      "Epoch 6 Batch 1700 Loss 1.0605 Accuracy 0.4814\n",
      "Epoch 6 Batch 1750 Loss 1.0581 Accuracy 0.4823\n",
      "Epoch 6 Batch 1800 Loss 1.0561 Accuracy 0.4831\n",
      "Epoch 6 Batch 1850 Loss 1.0533 Accuracy 0.4839\n",
      "Epoch 6 Batch 1900 Loss 1.0513 Accuracy 0.4847\n",
      "Epoch 6 Batch 1950 Loss 1.0498 Accuracy 0.4853\n",
      "Epoch 6 Batch 2000 Loss 1.0482 Accuracy 0.4859\n",
      "Epoch 6 Batch 2050 Loss 1.0462 Accuracy 0.4863\n",
      "Epoch 6 Batch 2100 Loss 1.0434 Accuracy 0.4867\n",
      "Epoch 6 Batch 2150 Loss 1.0407 Accuracy 0.4870\n",
      "Epoch 6 Batch 2200 Loss 1.0375 Accuracy 0.4871\n",
      "Epoch 6 Batch 2250 Loss 1.0345 Accuracy 0.4872\n",
      "Epoch 6 Batch 2300 Loss 1.0317 Accuracy 0.4874\n",
      "Epoch 6 Batch 2350 Loss 1.0288 Accuracy 0.4878\n",
      "Epoch 6 Batch 2400 Loss 1.0258 Accuracy 0.4880\n",
      "Epoch 6 Batch 2450 Loss 1.0231 Accuracy 0.4882\n",
      "Epoch 6 Batch 2500 Loss 1.0202 Accuracy 0.4885\n",
      "Epoch 6 Batch 2550 Loss 1.0173 Accuracy 0.4890\n",
      "Epoch 6 Batch 2600 Loss 1.0144 Accuracy 0.4894\n",
      "Epoch 6 Batch 2650 Loss 1.0120 Accuracy 0.4898\n",
      "Epoch 6 Batch 2700 Loss 1.0095 Accuracy 0.4900\n",
      "Epoch 6 Batch 2750 Loss 1.0071 Accuracy 0.4903\n",
      "Epoch 6 Batch 2800 Loss 1.0047 Accuracy 0.4906\n",
      "Epoch 6 Batch 2850 Loss 1.0024 Accuracy 0.4909\n",
      "Epoch 6 Batch 2900 Loss 1.0000 Accuracy 0.4912\n",
      "Epoch 6 Batch 2950 Loss 0.9979 Accuracy 0.4915\n",
      "Epoch 6 Batch 3000 Loss 0.9961 Accuracy 0.4917\n",
      "Epoch 6 Batch 3050 Loss 0.9942 Accuracy 0.4920\n",
      "Epoch 6 Batch 3100 Loss 0.9921 Accuracy 0.4923\n",
      "Epoch 6 Batch 3150 Loss 0.9905 Accuracy 0.4925\n",
      "Epoch 6 Batch 3200 Loss 0.9880 Accuracy 0.4928\n",
      "Epoch 6 Batch 3250 Loss 0.9858 Accuracy 0.4930\n",
      "Epoch 6 Batch 3300 Loss 0.9839 Accuracy 0.4933\n",
      "Epoch 6 Batch 3350 Loss 0.9817 Accuracy 0.4936\n",
      "Epoch 6 Batch 3400 Loss 0.9800 Accuracy 0.4938\n",
      "Epoch 6 Batch 3450 Loss 0.9783 Accuracy 0.4942\n",
      "Epoch 6 Batch 3500 Loss 0.9764 Accuracy 0.4944\n",
      "Epoch 6 Batch 3550 Loss 0.9745 Accuracy 0.4948\n",
      "Epoch 6 Batch 3600 Loss 0.9726 Accuracy 0.4951\n",
      "Epoch 6 Batch 3650 Loss 0.9710 Accuracy 0.4955\n",
      "Epoch 6 Batch 3700 Loss 0.9694 Accuracy 0.4958\n",
      "Epoch 6 Batch 3750 Loss 0.9678 Accuracy 0.4962\n",
      "Epoch 6 Batch 3800 Loss 0.9662 Accuracy 0.4965\n",
      "Epoch 6 Batch 3850 Loss 0.9647 Accuracy 0.4968\n",
      "Epoch 6 Batch 3900 Loss 0.9633 Accuracy 0.4971\n",
      "Epoch 6 Batch 3950 Loss 0.9617 Accuracy 0.4974\n",
      "Epoch 6 Batch 4000 Loss 0.9603 Accuracy 0.4978\n",
      "Epoch 6 Batch 4050 Loss 0.9591 Accuracy 0.4981\n",
      "Epoch 6 Batch 4100 Loss 0.9581 Accuracy 0.4984\n",
      "Epoch 6 Batch 4150 Loss 0.9578 Accuracy 0.4985\n",
      "Epoch 6 Batch 4200 Loss 0.9579 Accuracy 0.4985\n",
      "Epoch 6 Batch 4250 Loss 0.9581 Accuracy 0.4985\n",
      "Epoch 6 Batch 4300 Loss 0.9589 Accuracy 0.4984\n",
      "Epoch 6 Batch 4350 Loss 0.9599 Accuracy 0.4982\n",
      "Epoch 6 Batch 4400 Loss 0.9610 Accuracy 0.4981\n",
      "Epoch 6 Batch 4450 Loss 0.9623 Accuracy 0.4979\n",
      "Epoch 6 Batch 4500 Loss 0.9638 Accuracy 0.4977\n",
      "Epoch 6 Batch 4550 Loss 0.9649 Accuracy 0.4974\n",
      "Epoch 6 Batch 4600 Loss 0.9662 Accuracy 0.4973\n",
      "Epoch 6 Batch 4650 Loss 0.9676 Accuracy 0.4971\n",
      "Epoch 6 Batch 4700 Loss 0.9690 Accuracy 0.4969\n",
      "Epoch 6 Batch 4750 Loss 0.9702 Accuracy 0.4967\n",
      "Epoch 6 Batch 4800 Loss 0.9715 Accuracy 0.4966\n",
      "Epoch 6 Batch 4850 Loss 0.9725 Accuracy 0.4964\n",
      "Epoch 6 Batch 4900 Loss 0.9739 Accuracy 0.4962\n",
      "Epoch 6 Batch 4950 Loss 0.9750 Accuracy 0.4960\n",
      "Epoch 6 Batch 5000 Loss 0.9763 Accuracy 0.4958\n",
      "Epoch 6 Batch 5050 Loss 0.9776 Accuracy 0.4956\n",
      "Epoch 6 Batch 5100 Loss 0.9786 Accuracy 0.4954\n",
      "Epoch 6 Batch 5150 Loss 0.9802 Accuracy 0.4951\n",
      "Epoch 6 Batch 5200 Loss 0.9813 Accuracy 0.4949\n",
      "Epoch 6 Batch 5250 Loss 0.9824 Accuracy 0.4947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 5300 Loss 0.9837 Accuracy 0.4944\n",
      "Epoch 6 Batch 5350 Loss 0.9847 Accuracy 0.4941\n",
      "Epoch 6 Batch 5400 Loss 0.9858 Accuracy 0.4939\n",
      "Epoch 6 Batch 5450 Loss 0.9868 Accuracy 0.4936\n",
      "Epoch 6 Batch 5500 Loss 0.9877 Accuracy 0.4933\n",
      "Epoch 6 Batch 5550 Loss 0.9886 Accuracy 0.4931\n",
      "Epoch 6 Batch 5600 Loss 0.9893 Accuracy 0.4928\n",
      "Epoch 6 Batch 5650 Loss 0.9902 Accuracy 0.4926\n",
      "Epoch 6 Batch 5700 Loss 0.9910 Accuracy 0.4924\n",
      "Saving checkpoint for epoch 6 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-6\n",
      "Time taken for 1 epoch: 8420.899336576462 secs\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch 7 Batch 0 Loss 1.0138 Accuracy 0.4975\n",
      "Epoch 7 Batch 50 Loss 1.1123 Accuracy 0.4771\n",
      "Epoch 7 Batch 100 Loss 1.0889 Accuracy 0.4796\n",
      "Epoch 7 Batch 150 Loss 1.0967 Accuracy 0.4765\n",
      "Epoch 7 Batch 200 Loss 1.0890 Accuracy 0.4765\n",
      "Epoch 7 Batch 250 Loss 1.0868 Accuracy 0.4769\n",
      "Epoch 7 Batch 300 Loss 1.0801 Accuracy 0.4773\n",
      "Epoch 7 Batch 350 Loss 1.0783 Accuracy 0.4765\n",
      "Epoch 7 Batch 400 Loss 1.0784 Accuracy 0.4768\n",
      "Epoch 7 Batch 450 Loss 1.0774 Accuracy 0.4772\n",
      "Epoch 7 Batch 500 Loss 1.0763 Accuracy 0.4772\n",
      "Epoch 7 Batch 550 Loss 1.0747 Accuracy 0.4771\n",
      "Epoch 7 Batch 600 Loss 1.0730 Accuracy 0.4771\n",
      "Epoch 7 Batch 650 Loss 1.0724 Accuracy 0.4773\n",
      "Epoch 7 Batch 700 Loss 1.0714 Accuracy 0.4778\n",
      "Epoch 7 Batch 750 Loss 1.0691 Accuracy 0.4777\n",
      "Epoch 7 Batch 800 Loss 1.0680 Accuracy 0.4780\n",
      "Epoch 7 Batch 850 Loss 1.0683 Accuracy 0.4786\n",
      "Epoch 7 Batch 900 Loss 1.0658 Accuracy 0.4787\n",
      "Epoch 7 Batch 950 Loss 1.0638 Accuracy 0.4788\n",
      "Epoch 7 Batch 1000 Loss 1.0610 Accuracy 0.4790\n",
      "Epoch 7 Batch 1050 Loss 1.0599 Accuracy 0.4790\n",
      "Epoch 7 Batch 1100 Loss 1.0581 Accuracy 0.4791\n",
      "Epoch 7 Batch 1150 Loss 1.0567 Accuracy 0.4792\n",
      "Epoch 7 Batch 1200 Loss 1.0558 Accuracy 0.4793\n",
      "Epoch 7 Batch 1250 Loss 1.0537 Accuracy 0.4797\n",
      "Epoch 7 Batch 1300 Loss 1.0518 Accuracy 0.4801\n",
      "Epoch 7 Batch 1350 Loss 1.0490 Accuracy 0.4806\n",
      "Epoch 7 Batch 1400 Loss 1.0464 Accuracy 0.4811\n",
      "Epoch 7 Batch 1450 Loss 1.0441 Accuracy 0.4817\n",
      "Epoch 7 Batch 1500 Loss 1.0412 Accuracy 0.4825\n",
      "Epoch 7 Batch 1550 Loss 1.0387 Accuracy 0.4832\n",
      "Epoch 7 Batch 1600 Loss 1.0371 Accuracy 0.4840\n",
      "Epoch 7 Batch 1650 Loss 1.0348 Accuracy 0.4849\n",
      "Epoch 7 Batch 1700 Loss 1.0332 Accuracy 0.4858\n",
      "Epoch 7 Batch 1750 Loss 1.0304 Accuracy 0.4866\n",
      "Epoch 7 Batch 1800 Loss 1.0282 Accuracy 0.4874\n",
      "Epoch 7 Batch 1850 Loss 1.0257 Accuracy 0.4882\n",
      "Epoch 7 Batch 1900 Loss 1.0238 Accuracy 0.4891\n",
      "Epoch 7 Batch 1950 Loss 1.0216 Accuracy 0.4899\n",
      "Epoch 7 Batch 2000 Loss 1.0191 Accuracy 0.4905\n",
      "Epoch 7 Batch 2050 Loss 1.0168 Accuracy 0.4909\n",
      "Epoch 7 Batch 2100 Loss 1.0140 Accuracy 0.4913\n",
      "Epoch 7 Batch 2150 Loss 1.0115 Accuracy 0.4917\n",
      "Epoch 7 Batch 2200 Loss 1.0083 Accuracy 0.4918\n",
      "Epoch 7 Batch 2250 Loss 1.0054 Accuracy 0.4919\n",
      "Epoch 7 Batch 2300 Loss 1.0026 Accuracy 0.4922\n",
      "Epoch 7 Batch 2350 Loss 0.9998 Accuracy 0.4925\n",
      "Epoch 7 Batch 2400 Loss 0.9970 Accuracy 0.4926\n",
      "Epoch 7 Batch 2450 Loss 0.9944 Accuracy 0.4929\n",
      "Epoch 7 Batch 2500 Loss 0.9914 Accuracy 0.4932\n",
      "Epoch 7 Batch 2550 Loss 0.9887 Accuracy 0.4936\n",
      "Epoch 7 Batch 2600 Loss 0.9858 Accuracy 0.4940\n",
      "Epoch 7 Batch 2650 Loss 0.9832 Accuracy 0.4943\n",
      "Epoch 7 Batch 2700 Loss 0.9807 Accuracy 0.4945\n",
      "Epoch 7 Batch 2750 Loss 0.9785 Accuracy 0.4949\n",
      "Epoch 7 Batch 2800 Loss 0.9762 Accuracy 0.4952\n",
      "Epoch 7 Batch 2850 Loss 0.9738 Accuracy 0.4956\n",
      "Epoch 7 Batch 2900 Loss 0.9720 Accuracy 0.4959\n",
      "Epoch 7 Batch 2950 Loss 0.9700 Accuracy 0.4961\n",
      "Epoch 7 Batch 3000 Loss 0.9682 Accuracy 0.4963\n",
      "Epoch 7 Batch 3050 Loss 0.9661 Accuracy 0.4967\n",
      "Epoch 7 Batch 3100 Loss 0.9640 Accuracy 0.4969\n",
      "Epoch 7 Batch 3150 Loss 0.9623 Accuracy 0.4972\n",
      "Epoch 7 Batch 3200 Loss 0.9604 Accuracy 0.4975\n",
      "Epoch 7 Batch 3250 Loss 0.9581 Accuracy 0.4978\n",
      "Epoch 7 Batch 3300 Loss 0.9561 Accuracy 0.4979\n",
      "Epoch 7 Batch 3350 Loss 0.9539 Accuracy 0.4983\n",
      "Epoch 7 Batch 3400 Loss 0.9520 Accuracy 0.4986\n",
      "Epoch 7 Batch 3450 Loss 0.9505 Accuracy 0.4988\n",
      "Epoch 7 Batch 3500 Loss 0.9486 Accuracy 0.4991\n",
      "Epoch 7 Batch 3550 Loss 0.9466 Accuracy 0.4994\n",
      "Epoch 7 Batch 3600 Loss 0.9450 Accuracy 0.4997\n",
      "Epoch 7 Batch 3650 Loss 0.9434 Accuracy 0.5001\n",
      "Epoch 7 Batch 3700 Loss 0.9418 Accuracy 0.5004\n",
      "Epoch 7 Batch 3750 Loss 0.9403 Accuracy 0.5007\n",
      "Epoch 7 Batch 3800 Loss 0.9386 Accuracy 0.5010\n",
      "Epoch 7 Batch 3850 Loss 0.9371 Accuracy 0.5013\n",
      "Epoch 7 Batch 3900 Loss 0.9355 Accuracy 0.5016\n",
      "Epoch 7 Batch 3950 Loss 0.9343 Accuracy 0.5020\n",
      "Epoch 7 Batch 4000 Loss 0.9332 Accuracy 0.5022\n",
      "Epoch 7 Batch 4050 Loss 0.9318 Accuracy 0.5025\n",
      "Epoch 7 Batch 4100 Loss 0.9309 Accuracy 0.5027\n",
      "Epoch 7 Batch 4150 Loss 0.9307 Accuracy 0.5028\n",
      "Epoch 7 Batch 4200 Loss 0.9308 Accuracy 0.5028\n",
      "Epoch 7 Batch 4250 Loss 0.9315 Accuracy 0.5027\n",
      "Epoch 7 Batch 4300 Loss 0.9321 Accuracy 0.5027\n",
      "Epoch 7 Batch 4350 Loss 0.9331 Accuracy 0.5026\n",
      "Epoch 7 Batch 4400 Loss 0.9341 Accuracy 0.5025\n",
      "Epoch 7 Batch 4450 Loss 0.9352 Accuracy 0.5023\n",
      "Epoch 7 Batch 4500 Loss 0.9365 Accuracy 0.5022\n",
      "Epoch 7 Batch 4550 Loss 0.9379 Accuracy 0.5020\n",
      "Epoch 7 Batch 4600 Loss 0.9393 Accuracy 0.5019\n",
      "Epoch 7 Batch 4650 Loss 0.9407 Accuracy 0.5016\n",
      "Epoch 7 Batch 4700 Loss 0.9418 Accuracy 0.5014\n",
      "Epoch 7 Batch 4750 Loss 0.9428 Accuracy 0.5012\n",
      "Epoch 7 Batch 4800 Loss 0.9441 Accuracy 0.5011\n",
      "Epoch 7 Batch 4850 Loss 0.9454 Accuracy 0.5009\n",
      "Epoch 7 Batch 4900 Loss 0.9470 Accuracy 0.5007\n",
      "Epoch 7 Batch 4950 Loss 0.9479 Accuracy 0.5006\n",
      "Epoch 7 Batch 5000 Loss 0.9491 Accuracy 0.5004\n",
      "Epoch 7 Batch 5050 Loss 0.9503 Accuracy 0.5001\n",
      "Epoch 7 Batch 5100 Loss 0.9517 Accuracy 0.4998\n",
      "Epoch 7 Batch 5150 Loss 0.9530 Accuracy 0.4996\n",
      "Epoch 7 Batch 5200 Loss 0.9542 Accuracy 0.4993\n",
      "Epoch 7 Batch 5250 Loss 0.9554 Accuracy 0.4991\n",
      "Epoch 7 Batch 5300 Loss 0.9566 Accuracy 0.4988\n",
      "Epoch 7 Batch 5350 Loss 0.9576 Accuracy 0.4985\n",
      "Epoch 7 Batch 5400 Loss 0.9586 Accuracy 0.4983\n",
      "Epoch 7 Batch 5450 Loss 0.9599 Accuracy 0.4980\n",
      "Epoch 7 Batch 5500 Loss 0.9607 Accuracy 0.4978\n",
      "Epoch 7 Batch 5550 Loss 0.9616 Accuracy 0.4975\n",
      "Epoch 7 Batch 5600 Loss 0.9627 Accuracy 0.4973\n",
      "Epoch 7 Batch 5650 Loss 0.9635 Accuracy 0.4971\n",
      "Epoch 7 Batch 5700 Loss 0.9643 Accuracy 0.4968\n",
      "Saving checkpoint for epoch 7 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-7\n",
      "Time taken for 1 epoch: 8429.940502882004 secs\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 Batch 0 Loss 1.1822 Accuracy 0.4901\n",
      "Epoch 8 Batch 50 Loss 1.0504 Accuracy 0.4811\n",
      "Epoch 8 Batch 100 Loss 1.0523 Accuracy 0.4843\n",
      "Epoch 8 Batch 150 Loss 1.0550 Accuracy 0.4827\n",
      "Epoch 8 Batch 200 Loss 1.0555 Accuracy 0.4827\n",
      "Epoch 8 Batch 250 Loss 1.0586 Accuracy 0.4818\n",
      "Epoch 8 Batch 300 Loss 1.0573 Accuracy 0.4816\n",
      "Epoch 8 Batch 350 Loss 1.0519 Accuracy 0.4816\n",
      "Epoch 8 Batch 400 Loss 1.0491 Accuracy 0.4806\n",
      "Epoch 8 Batch 450 Loss 1.0472 Accuracy 0.4811\n",
      "Epoch 8 Batch 500 Loss 1.0464 Accuracy 0.4808\n",
      "Epoch 8 Batch 550 Loss 1.0450 Accuracy 0.4809\n",
      "Epoch 8 Batch 600 Loss 1.0453 Accuracy 0.4813\n",
      "Epoch 8 Batch 650 Loss 1.0445 Accuracy 0.4810\n",
      "Epoch 8 Batch 700 Loss 1.0420 Accuracy 0.4816\n",
      "Epoch 8 Batch 750 Loss 1.0416 Accuracy 0.4820\n",
      "Epoch 8 Batch 800 Loss 1.0431 Accuracy 0.4820\n",
      "Epoch 8 Batch 850 Loss 1.0410 Accuracy 0.4826\n",
      "Epoch 8 Batch 900 Loss 1.0390 Accuracy 0.4825\n",
      "Epoch 8 Batch 950 Loss 1.0377 Accuracy 0.4824\n",
      "Epoch 8 Batch 1000 Loss 1.0367 Accuracy 0.4824\n",
      "Epoch 8 Batch 1050 Loss 1.0361 Accuracy 0.4825\n",
      "Epoch 8 Batch 1100 Loss 1.0354 Accuracy 0.4829\n",
      "Epoch 8 Batch 1150 Loss 1.0334 Accuracy 0.4830\n",
      "Epoch 8 Batch 1200 Loss 1.0320 Accuracy 0.4833\n",
      "Epoch 8 Batch 1250 Loss 1.0300 Accuracy 0.4835\n",
      "Epoch 8 Batch 1300 Loss 1.0285 Accuracy 0.4840\n",
      "Epoch 8 Batch 1350 Loss 1.0270 Accuracy 0.4844\n",
      "Epoch 8 Batch 1400 Loss 1.0248 Accuracy 0.4851\n",
      "Epoch 8 Batch 1450 Loss 1.0223 Accuracy 0.4857\n",
      "Epoch 8 Batch 1500 Loss 1.0194 Accuracy 0.4864\n",
      "Epoch 8 Batch 1550 Loss 1.0171 Accuracy 0.4872\n",
      "Epoch 8 Batch 1600 Loss 1.0143 Accuracy 0.4881\n",
      "Epoch 8 Batch 1650 Loss 1.0118 Accuracy 0.4890\n",
      "Epoch 8 Batch 1700 Loss 1.0095 Accuracy 0.4899\n",
      "Epoch 8 Batch 1750 Loss 1.0073 Accuracy 0.4906\n",
      "Epoch 8 Batch 1800 Loss 1.0051 Accuracy 0.4914\n",
      "Epoch 8 Batch 1850 Loss 1.0024 Accuracy 0.4922\n",
      "Epoch 8 Batch 1900 Loss 1.0001 Accuracy 0.4931\n",
      "Epoch 8 Batch 1950 Loss 0.9978 Accuracy 0.4937\n",
      "Epoch 8 Batch 2000 Loss 0.9956 Accuracy 0.4943\n",
      "Epoch 8 Batch 2050 Loss 0.9935 Accuracy 0.4948\n",
      "Epoch 8 Batch 2100 Loss 0.9912 Accuracy 0.4952\n",
      "Epoch 8 Batch 2150 Loss 0.9885 Accuracy 0.4955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 2200 Loss 0.9857 Accuracy 0.4955\n",
      "Epoch 8 Batch 2250 Loss 0.9832 Accuracy 0.4958\n",
      "Epoch 8 Batch 2300 Loss 0.9803 Accuracy 0.4961\n",
      "Epoch 8 Batch 2350 Loss 0.9777 Accuracy 0.4964\n",
      "Epoch 8 Batch 2400 Loss 0.9752 Accuracy 0.4966\n",
      "Epoch 8 Batch 2450 Loss 0.9722 Accuracy 0.4968\n",
      "Epoch 8 Batch 2500 Loss 0.9690 Accuracy 0.4971\n",
      "Epoch 8 Batch 2550 Loss 0.9661 Accuracy 0.4975\n",
      "Epoch 8 Batch 2600 Loss 0.9631 Accuracy 0.4978\n",
      "Epoch 8 Batch 2650 Loss 0.9602 Accuracy 0.4981\n",
      "Epoch 8 Batch 2700 Loss 0.9577 Accuracy 0.4985\n",
      "Epoch 8 Batch 2750 Loss 0.9552 Accuracy 0.4988\n",
      "Epoch 8 Batch 2800 Loss 0.9531 Accuracy 0.4992\n",
      "Epoch 8 Batch 2850 Loss 0.9513 Accuracy 0.4995\n",
      "Epoch 8 Batch 2900 Loss 0.9496 Accuracy 0.4997\n",
      "Epoch 8 Batch 2950 Loss 0.9474 Accuracy 0.5000\n",
      "Epoch 8 Batch 3000 Loss 0.9459 Accuracy 0.5002\n",
      "Epoch 8 Batch 3050 Loss 0.9438 Accuracy 0.5004\n",
      "Epoch 8 Batch 3100 Loss 0.9417 Accuracy 0.5007\n",
      "Epoch 8 Batch 3150 Loss 0.9401 Accuracy 0.5009\n",
      "Epoch 8 Batch 3200 Loss 0.9383 Accuracy 0.5012\n",
      "Epoch 8 Batch 3250 Loss 0.9365 Accuracy 0.5014\n",
      "Epoch 8 Batch 3300 Loss 0.9344 Accuracy 0.5016\n",
      "Epoch 8 Batch 3350 Loss 0.9322 Accuracy 0.5019\n",
      "Epoch 8 Batch 3400 Loss 0.9307 Accuracy 0.5022\n",
      "Epoch 8 Batch 3450 Loss 0.9289 Accuracy 0.5025\n",
      "Epoch 8 Batch 3500 Loss 0.9271 Accuracy 0.5027\n",
      "Epoch 8 Batch 3550 Loss 0.9253 Accuracy 0.5031\n",
      "Epoch 8 Batch 3600 Loss 0.9236 Accuracy 0.5033\n",
      "Epoch 8 Batch 3650 Loss 0.9220 Accuracy 0.5037\n",
      "Epoch 8 Batch 3700 Loss 0.9201 Accuracy 0.5041\n",
      "Epoch 8 Batch 3750 Loss 0.9185 Accuracy 0.5044\n",
      "Epoch 8 Batch 3800 Loss 0.9171 Accuracy 0.5046\n",
      "Epoch 8 Batch 3850 Loss 0.9159 Accuracy 0.5049\n",
      "Epoch 8 Batch 3900 Loss 0.9145 Accuracy 0.5052\n",
      "Epoch 8 Batch 3950 Loss 0.9130 Accuracy 0.5056\n",
      "Epoch 8 Batch 4000 Loss 0.9119 Accuracy 0.5059\n",
      "Epoch 8 Batch 4050 Loss 0.9109 Accuracy 0.5061\n",
      "Epoch 8 Batch 4100 Loss 0.9099 Accuracy 0.5064\n",
      "Epoch 8 Batch 4150 Loss 0.9097 Accuracy 0.5065\n",
      "Epoch 8 Batch 4200 Loss 0.9097 Accuracy 0.5065\n",
      "Epoch 8 Batch 4250 Loss 0.9101 Accuracy 0.5064\n",
      "Epoch 8 Batch 4300 Loss 0.9107 Accuracy 0.5064\n",
      "Epoch 8 Batch 4350 Loss 0.9116 Accuracy 0.5062\n",
      "Epoch 8 Batch 4400 Loss 0.9125 Accuracy 0.5062\n",
      "Epoch 8 Batch 4450 Loss 0.9137 Accuracy 0.5060\n",
      "Epoch 8 Batch 4500 Loss 0.9150 Accuracy 0.5057\n",
      "Epoch 8 Batch 4550 Loss 0.9162 Accuracy 0.5055\n",
      "Epoch 8 Batch 4600 Loss 0.9177 Accuracy 0.5053\n",
      "Epoch 8 Batch 4650 Loss 0.9194 Accuracy 0.5050\n",
      "Epoch 8 Batch 4700 Loss 0.9210 Accuracy 0.5049\n",
      "Epoch 8 Batch 4750 Loss 0.9224 Accuracy 0.5047\n",
      "Epoch 8 Batch 4800 Loss 0.9238 Accuracy 0.5046\n",
      "Epoch 8 Batch 4850 Loss 0.9252 Accuracy 0.5044\n",
      "Epoch 8 Batch 4900 Loss 0.9261 Accuracy 0.5043\n",
      "Epoch 8 Batch 4950 Loss 0.9273 Accuracy 0.5041\n",
      "Epoch 8 Batch 5000 Loss 0.9286 Accuracy 0.5038\n",
      "Epoch 8 Batch 5050 Loss 0.9298 Accuracy 0.5036\n",
      "Epoch 8 Batch 5100 Loss 0.9312 Accuracy 0.5033\n",
      "Epoch 8 Batch 5150 Loss 0.9325 Accuracy 0.5031\n",
      "Epoch 8 Batch 5200 Loss 0.9337 Accuracy 0.5028\n",
      "Epoch 8 Batch 5250 Loss 0.9350 Accuracy 0.5026\n",
      "Epoch 8 Batch 5300 Loss 0.9362 Accuracy 0.5022\n",
      "Epoch 8 Batch 5350 Loss 0.9371 Accuracy 0.5020\n",
      "Epoch 8 Batch 5400 Loss 0.9380 Accuracy 0.5017\n",
      "Epoch 8 Batch 5450 Loss 0.9391 Accuracy 0.5015\n",
      "Epoch 8 Batch 5500 Loss 0.9403 Accuracy 0.5012\n",
      "Epoch 8 Batch 5550 Loss 0.9412 Accuracy 0.5009\n",
      "Epoch 8 Batch 5600 Loss 0.9422 Accuracy 0.5007\n",
      "Epoch 8 Batch 5650 Loss 0.9430 Accuracy 0.5005\n",
      "Epoch 8 Batch 5700 Loss 0.9438 Accuracy 0.5002\n",
      "Saving checkpoint for epoch 8 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-8\n",
      "Time taken for 1 epoch: 7039.145644426346 secs\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 Batch 0 Loss 0.8947 Accuracy 0.5132\n",
      "Epoch 9 Batch 50 Loss 1.0612 Accuracy 0.4796\n",
      "Epoch 9 Batch 100 Loss 1.0587 Accuracy 0.4804\n",
      "Epoch 9 Batch 150 Loss 1.0497 Accuracy 0.4832\n",
      "Epoch 9 Batch 200 Loss 1.0466 Accuracy 0.4831\n",
      "Epoch 9 Batch 250 Loss 1.0443 Accuracy 0.4846\n",
      "Epoch 9 Batch 300 Loss 1.0420 Accuracy 0.4843\n",
      "Epoch 9 Batch 350 Loss 1.0380 Accuracy 0.4847\n",
      "Epoch 9 Batch 400 Loss 1.0352 Accuracy 0.4847\n",
      "Epoch 9 Batch 450 Loss 1.0325 Accuracy 0.4843\n",
      "Epoch 9 Batch 500 Loss 1.0321 Accuracy 0.4841\n",
      "Epoch 9 Batch 550 Loss 1.0321 Accuracy 0.4841\n",
      "Epoch 9 Batch 600 Loss 1.0310 Accuracy 0.4840\n",
      "Epoch 9 Batch 650 Loss 1.0284 Accuracy 0.4849\n",
      "Epoch 9 Batch 700 Loss 1.0264 Accuracy 0.4853\n",
      "Epoch 9 Batch 750 Loss 1.0257 Accuracy 0.4855\n",
      "Epoch 9 Batch 800 Loss 1.0245 Accuracy 0.4855\n",
      "Epoch 9 Batch 850 Loss 1.0234 Accuracy 0.4854\n",
      "Epoch 9 Batch 900 Loss 1.0225 Accuracy 0.4852\n",
      "Epoch 9 Batch 950 Loss 1.0213 Accuracy 0.4854\n",
      "Epoch 9 Batch 1000 Loss 1.0194 Accuracy 0.4852\n",
      "Epoch 9 Batch 1050 Loss 1.0186 Accuracy 0.4854\n",
      "Epoch 9 Batch 1100 Loss 1.0183 Accuracy 0.4852\n",
      "Epoch 9 Batch 1150 Loss 1.0169 Accuracy 0.4854\n",
      "Epoch 9 Batch 1200 Loss 1.0154 Accuracy 0.4857\n",
      "Epoch 9 Batch 1250 Loss 1.0124 Accuracy 0.4858\n",
      "Epoch 9 Batch 1300 Loss 1.0107 Accuracy 0.4863\n",
      "Epoch 9 Batch 1350 Loss 1.0084 Accuracy 0.4870\n",
      "Epoch 9 Batch 1400 Loss 1.0062 Accuracy 0.4879\n",
      "Epoch 9 Batch 1450 Loss 1.0041 Accuracy 0.4887\n",
      "Epoch 9 Batch 1500 Loss 1.0014 Accuracy 0.4896\n",
      "Epoch 9 Batch 1550 Loss 0.9984 Accuracy 0.4903\n",
      "Epoch 9 Batch 1600 Loss 0.9961 Accuracy 0.4912\n",
      "Epoch 9 Batch 1650 Loss 0.9936 Accuracy 0.4922\n",
      "Epoch 9 Batch 1700 Loss 0.9916 Accuracy 0.4930\n",
      "Epoch 9 Batch 1750 Loss 0.9897 Accuracy 0.4938\n",
      "Epoch 9 Batch 1800 Loss 0.9871 Accuracy 0.4946\n",
      "Epoch 9 Batch 1850 Loss 0.9850 Accuracy 0.4955\n",
      "Epoch 9 Batch 1900 Loss 0.9824 Accuracy 0.4962\n",
      "Epoch 9 Batch 1950 Loss 0.9797 Accuracy 0.4969\n",
      "Epoch 9 Batch 2000 Loss 0.9775 Accuracy 0.4975\n",
      "Epoch 9 Batch 2050 Loss 0.9755 Accuracy 0.4980\n",
      "Epoch 9 Batch 2100 Loss 0.9735 Accuracy 0.4983\n",
      "Epoch 9 Batch 2150 Loss 0.9709 Accuracy 0.4986\n",
      "Epoch 9 Batch 2200 Loss 0.9684 Accuracy 0.4989\n",
      "Epoch 9 Batch 2250 Loss 0.9648 Accuracy 0.4989\n",
      "Epoch 9 Batch 2300 Loss 0.9621 Accuracy 0.4993\n",
      "Epoch 9 Batch 2350 Loss 0.9595 Accuracy 0.4994\n",
      "Epoch 9 Batch 2400 Loss 0.9566 Accuracy 0.4996\n",
      "Epoch 9 Batch 2450 Loss 0.9539 Accuracy 0.4998\n",
      "Epoch 9 Batch 2500 Loss 0.9514 Accuracy 0.5002\n",
      "Epoch 9 Batch 2550 Loss 0.9490 Accuracy 0.5005\n",
      "Epoch 9 Batch 2600 Loss 0.9458 Accuracy 0.5008\n",
      "Epoch 9 Batch 2650 Loss 0.9432 Accuracy 0.5012\n",
      "Epoch 9 Batch 2700 Loss 0.9405 Accuracy 0.5014\n",
      "Epoch 9 Batch 2750 Loss 0.9387 Accuracy 0.5018\n",
      "Epoch 9 Batch 2800 Loss 0.9365 Accuracy 0.5020\n",
      "Epoch 9 Batch 2850 Loss 0.9340 Accuracy 0.5024\n",
      "Epoch 9 Batch 2900 Loss 0.9320 Accuracy 0.5027\n",
      "Epoch 9 Batch 2950 Loss 0.9300 Accuracy 0.5028\n",
      "Epoch 9 Batch 3000 Loss 0.9281 Accuracy 0.5030\n",
      "Epoch 9 Batch 3050 Loss 0.9260 Accuracy 0.5034\n",
      "Epoch 9 Batch 3100 Loss 0.9246 Accuracy 0.5036\n",
      "Epoch 9 Batch 3150 Loss 0.9227 Accuracy 0.5040\n",
      "Epoch 9 Batch 3200 Loss 0.9208 Accuracy 0.5042\n",
      "Epoch 9 Batch 3250 Loss 0.9185 Accuracy 0.5045\n",
      "Epoch 9 Batch 3300 Loss 0.9168 Accuracy 0.5046\n",
      "Epoch 9 Batch 3350 Loss 0.9149 Accuracy 0.5049\n",
      "Epoch 9 Batch 3400 Loss 0.9129 Accuracy 0.5052\n",
      "Epoch 9 Batch 3450 Loss 0.9113 Accuracy 0.5054\n",
      "Epoch 9 Batch 3500 Loss 0.9099 Accuracy 0.5057\n",
      "Epoch 9 Batch 3550 Loss 0.9081 Accuracy 0.5059\n",
      "Epoch 9 Batch 3600 Loss 0.9063 Accuracy 0.5062\n",
      "Epoch 9 Batch 3650 Loss 0.9048 Accuracy 0.5066\n",
      "Epoch 9 Batch 3700 Loss 0.9031 Accuracy 0.5070\n",
      "Epoch 9 Batch 3750 Loss 0.9017 Accuracy 0.5072\n",
      "Epoch 9 Batch 3800 Loss 0.9001 Accuracy 0.5076\n",
      "Epoch 9 Batch 3850 Loss 0.8987 Accuracy 0.5078\n",
      "Epoch 9 Batch 3900 Loss 0.8977 Accuracy 0.5082\n",
      "Epoch 9 Batch 3950 Loss 0.8961 Accuracy 0.5085\n",
      "Epoch 9 Batch 4000 Loss 0.8951 Accuracy 0.5088\n",
      "Epoch 9 Batch 4050 Loss 0.8939 Accuracy 0.5091\n",
      "Epoch 9 Batch 4100 Loss 0.8927 Accuracy 0.5093\n",
      "Epoch 9 Batch 4150 Loss 0.8921 Accuracy 0.5094\n",
      "Epoch 9 Batch 4200 Loss 0.8923 Accuracy 0.5094\n",
      "Epoch 9 Batch 4250 Loss 0.8929 Accuracy 0.5093\n",
      "Epoch 9 Batch 4300 Loss 0.8938 Accuracy 0.5093\n",
      "Epoch 9 Batch 4350 Loss 0.8946 Accuracy 0.5092\n",
      "Epoch 9 Batch 4400 Loss 0.8955 Accuracy 0.5091\n",
      "Epoch 9 Batch 4450 Loss 0.8966 Accuracy 0.5090\n",
      "Epoch 9 Batch 4500 Loss 0.8977 Accuracy 0.5087\n",
      "Epoch 9 Batch 4550 Loss 0.8992 Accuracy 0.5085\n",
      "Epoch 9 Batch 4600 Loss 0.9006 Accuracy 0.5082\n",
      "Epoch 9 Batch 4650 Loss 0.9018 Accuracy 0.5080\n",
      "Epoch 9 Batch 4700 Loss 0.9032 Accuracy 0.5078\n",
      "Epoch 9 Batch 4750 Loss 0.9044 Accuracy 0.5076\n",
      "Epoch 9 Batch 4800 Loss 0.9057 Accuracy 0.5074\n",
      "Epoch 9 Batch 4850 Loss 0.9070 Accuracy 0.5073\n",
      "Epoch 9 Batch 4900 Loss 0.9082 Accuracy 0.5072\n",
      "Epoch 9 Batch 4950 Loss 0.9097 Accuracy 0.5069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 5000 Loss 0.9110 Accuracy 0.5067\n",
      "Epoch 9 Batch 5050 Loss 0.9123 Accuracy 0.5064\n",
      "Epoch 9 Batch 5100 Loss 0.9136 Accuracy 0.5062\n",
      "Epoch 9 Batch 5150 Loss 0.9148 Accuracy 0.5059\n",
      "Epoch 9 Batch 5200 Loss 0.9161 Accuracy 0.5056\n",
      "Epoch 9 Batch 5250 Loss 0.9174 Accuracy 0.5054\n",
      "Epoch 9 Batch 5300 Loss 0.9185 Accuracy 0.5051\n",
      "Epoch 9 Batch 5350 Loss 0.9196 Accuracy 0.5048\n",
      "Epoch 9 Batch 5400 Loss 0.9210 Accuracy 0.5046\n",
      "Epoch 9 Batch 5450 Loss 0.9223 Accuracy 0.5043\n",
      "Epoch 9 Batch 5500 Loss 0.9232 Accuracy 0.5040\n",
      "Epoch 9 Batch 5550 Loss 0.9242 Accuracy 0.5038\n",
      "Epoch 9 Batch 5600 Loss 0.9252 Accuracy 0.5035\n",
      "Epoch 9 Batch 5650 Loss 0.9261 Accuracy 0.5033\n",
      "Epoch 9 Batch 5700 Loss 0.9270 Accuracy 0.5030\n",
      "Saving checkpoint for epoch 9 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-9\n",
      "Time taken for 1 epoch: 6631.252534627914 secs\n",
      "\n",
      "Start of epoch 10\n",
      "Epoch 10 Batch 0 Loss 0.9803 Accuracy 0.4753\n",
      "Epoch 10 Batch 50 Loss 1.0170 Accuracy 0.4885\n",
      "Epoch 10 Batch 100 Loss 1.0221 Accuracy 0.4886\n",
      "Epoch 10 Batch 150 Loss 1.0190 Accuracy 0.4879\n",
      "Epoch 10 Batch 200 Loss 1.0175 Accuracy 0.4873\n",
      "Epoch 10 Batch 250 Loss 1.0196 Accuracy 0.4866\n",
      "Epoch 10 Batch 300 Loss 1.0144 Accuracy 0.4864\n",
      "Epoch 10 Batch 350 Loss 1.0145 Accuracy 0.4862\n",
      "Epoch 10 Batch 400 Loss 1.0131 Accuracy 0.4863\n",
      "Epoch 10 Batch 450 Loss 1.0107 Accuracy 0.4865\n",
      "Epoch 10 Batch 500 Loss 1.0107 Accuracy 0.4864\n",
      "Epoch 10 Batch 550 Loss 1.0083 Accuracy 0.4862\n",
      "Epoch 10 Batch 600 Loss 1.0104 Accuracy 0.4861\n",
      "Epoch 10 Batch 650 Loss 1.0103 Accuracy 0.4862\n",
      "Epoch 10 Batch 700 Loss 1.0103 Accuracy 0.4869\n",
      "Epoch 10 Batch 750 Loss 1.0109 Accuracy 0.4874\n",
      "Epoch 10 Batch 800 Loss 1.0087 Accuracy 0.4877\n",
      "Epoch 10 Batch 850 Loss 1.0085 Accuracy 0.4877\n",
      "Epoch 10 Batch 900 Loss 1.0085 Accuracy 0.4881\n",
      "Epoch 10 Batch 950 Loss 1.0067 Accuracy 0.4884\n",
      "Epoch 10 Batch 1000 Loss 1.0048 Accuracy 0.4882\n",
      "Epoch 10 Batch 1050 Loss 1.0025 Accuracy 0.4882\n",
      "Epoch 10 Batch 1100 Loss 1.0004 Accuracy 0.4883\n",
      "Epoch 10 Batch 1150 Loss 0.9988 Accuracy 0.4885\n",
      "Epoch 10 Batch 1200 Loss 0.9976 Accuracy 0.4889\n",
      "Epoch 10 Batch 1250 Loss 0.9956 Accuracy 0.4893\n",
      "Epoch 10 Batch 1300 Loss 0.9934 Accuracy 0.4897\n",
      "Epoch 10 Batch 1350 Loss 0.9914 Accuracy 0.4902\n",
      "Epoch 10 Batch 1400 Loss 0.9891 Accuracy 0.4908\n",
      "Epoch 10 Batch 1450 Loss 0.9878 Accuracy 0.4915\n",
      "Epoch 10 Batch 1500 Loss 0.9845 Accuracy 0.4922\n",
      "Epoch 10 Batch 1550 Loss 0.9820 Accuracy 0.4930\n",
      "Epoch 10 Batch 1600 Loss 0.9791 Accuracy 0.4939\n",
      "Epoch 10 Batch 1650 Loss 0.9768 Accuracy 0.4948\n",
      "Epoch 10 Batch 1700 Loss 0.9743 Accuracy 0.4955\n",
      "Epoch 10 Batch 1750 Loss 0.9722 Accuracy 0.4963\n",
      "Epoch 10 Batch 1800 Loss 0.9696 Accuracy 0.4972\n",
      "Epoch 10 Batch 1850 Loss 0.9672 Accuracy 0.4980\n",
      "Epoch 10 Batch 1900 Loss 0.9654 Accuracy 0.4988\n",
      "Epoch 10 Batch 1950 Loss 0.9631 Accuracy 0.4997\n",
      "Epoch 10 Batch 2000 Loss 0.9608 Accuracy 0.5003\n",
      "Epoch 10 Batch 2050 Loss 0.9590 Accuracy 0.5007\n",
      "Epoch 10 Batch 2100 Loss 0.9570 Accuracy 0.5010\n",
      "Epoch 10 Batch 2150 Loss 0.9541 Accuracy 0.5013\n",
      "Epoch 10 Batch 2200 Loss 0.9511 Accuracy 0.5015\n",
      "Epoch 10 Batch 2250 Loss 0.9481 Accuracy 0.5016\n",
      "Epoch 10 Batch 2300 Loss 0.9454 Accuracy 0.5017\n",
      "Epoch 10 Batch 2350 Loss 0.9428 Accuracy 0.5019\n",
      "Epoch 10 Batch 2400 Loss 0.9397 Accuracy 0.5022\n",
      "Epoch 10 Batch 2450 Loss 0.9368 Accuracy 0.5023\n",
      "Epoch 10 Batch 2500 Loss 0.9336 Accuracy 0.5025\n",
      "Epoch 10 Batch 2550 Loss 0.9313 Accuracy 0.5028\n",
      "Epoch 10 Batch 2600 Loss 0.9287 Accuracy 0.5032\n",
      "Epoch 10 Batch 2650 Loss 0.9265 Accuracy 0.5036\n",
      "Epoch 10 Batch 2700 Loss 0.9242 Accuracy 0.5039\n",
      "Epoch 10 Batch 2750 Loss 0.9222 Accuracy 0.5043\n",
      "Epoch 10 Batch 2800 Loss 0.9202 Accuracy 0.5045\n",
      "Epoch 10 Batch 2850 Loss 0.9180 Accuracy 0.5048\n",
      "Epoch 10 Batch 2900 Loss 0.9160 Accuracy 0.5052\n",
      "Epoch 10 Batch 2950 Loss 0.9142 Accuracy 0.5054\n",
      "Epoch 10 Batch 3000 Loss 0.9124 Accuracy 0.5056\n",
      "Epoch 10 Batch 3050 Loss 0.9108 Accuracy 0.5059\n",
      "Epoch 10 Batch 3100 Loss 0.9088 Accuracy 0.5062\n",
      "Epoch 10 Batch 3150 Loss 0.9073 Accuracy 0.5065\n",
      "Epoch 10 Batch 3200 Loss 0.9057 Accuracy 0.5067\n",
      "Epoch 10 Batch 3250 Loss 0.9039 Accuracy 0.5068\n",
      "Epoch 10 Batch 3300 Loss 0.9021 Accuracy 0.5071\n",
      "Epoch 10 Batch 3350 Loss 0.9003 Accuracy 0.5074\n",
      "Epoch 10 Batch 3400 Loss 0.8984 Accuracy 0.5077\n",
      "Epoch 10 Batch 3450 Loss 0.8965 Accuracy 0.5079\n",
      "Epoch 10 Batch 3500 Loss 0.8946 Accuracy 0.5082\n",
      "Epoch 10 Batch 3550 Loss 0.8930 Accuracy 0.5086\n",
      "Epoch 10 Batch 3600 Loss 0.8914 Accuracy 0.5088\n",
      "Epoch 10 Batch 3650 Loss 0.8896 Accuracy 0.5090\n",
      "Epoch 10 Batch 3700 Loss 0.8878 Accuracy 0.5094\n",
      "Epoch 10 Batch 3750 Loss 0.8863 Accuracy 0.5097\n",
      "Epoch 10 Batch 3800 Loss 0.8849 Accuracy 0.5101\n",
      "Epoch 10 Batch 3850 Loss 0.8834 Accuracy 0.5103\n",
      "Epoch 10 Batch 3900 Loss 0.8822 Accuracy 0.5107\n",
      "Epoch 10 Batch 3950 Loss 0.8809 Accuracy 0.5110\n",
      "Epoch 10 Batch 4000 Loss 0.8797 Accuracy 0.5113\n",
      "Epoch 10 Batch 4050 Loss 0.8785 Accuracy 0.5116\n",
      "Epoch 10 Batch 4100 Loss 0.8774 Accuracy 0.5118\n",
      "Epoch 10 Batch 4150 Loss 0.8771 Accuracy 0.5119\n",
      "Epoch 10 Batch 4200 Loss 0.8775 Accuracy 0.5119\n",
      "Epoch 10 Batch 4250 Loss 0.8778 Accuracy 0.5119\n",
      "Epoch 10 Batch 4300 Loss 0.8785 Accuracy 0.5118\n",
      "Epoch 10 Batch 4350 Loss 0.8794 Accuracy 0.5117\n",
      "Epoch 10 Batch 4400 Loss 0.8807 Accuracy 0.5115\n",
      "Epoch 10 Batch 4450 Loss 0.8819 Accuracy 0.5113\n",
      "Epoch 10 Batch 4500 Loss 0.8831 Accuracy 0.5111\n",
      "Epoch 10 Batch 4550 Loss 0.8842 Accuracy 0.5110\n",
      "Epoch 10 Batch 4600 Loss 0.8854 Accuracy 0.5108\n",
      "Epoch 10 Batch 4650 Loss 0.8869 Accuracy 0.5106\n",
      "Epoch 10 Batch 4700 Loss 0.8882 Accuracy 0.5104\n",
      "Epoch 10 Batch 4750 Loss 0.8895 Accuracy 0.5102\n",
      "Epoch 10 Batch 4800 Loss 0.8908 Accuracy 0.5100\n",
      "Epoch 10 Batch 4850 Loss 0.8919 Accuracy 0.5098\n",
      "Epoch 10 Batch 4900 Loss 0.8934 Accuracy 0.5096\n",
      "Epoch 10 Batch 4950 Loss 0.8948 Accuracy 0.5094\n",
      "Epoch 10 Batch 5000 Loss 0.8959 Accuracy 0.5092\n",
      "Epoch 10 Batch 5050 Loss 0.8971 Accuracy 0.5090\n",
      "Epoch 10 Batch 5100 Loss 0.8984 Accuracy 0.5087\n",
      "Epoch 10 Batch 5150 Loss 0.8997 Accuracy 0.5085\n",
      "Epoch 10 Batch 5200 Loss 0.9010 Accuracy 0.5082\n",
      "Epoch 10 Batch 5250 Loss 0.9023 Accuracy 0.5079\n",
      "Epoch 10 Batch 5300 Loss 0.9033 Accuracy 0.5076\n",
      "Epoch 10 Batch 5350 Loss 0.9045 Accuracy 0.5073\n",
      "Epoch 10 Batch 5400 Loss 0.9057 Accuracy 0.5070\n",
      "Epoch 10 Batch 5450 Loss 0.9068 Accuracy 0.5068\n",
      "Epoch 10 Batch 5500 Loss 0.9076 Accuracy 0.5065\n",
      "Epoch 10 Batch 5550 Loss 0.9087 Accuracy 0.5063\n",
      "Epoch 10 Batch 5600 Loss 0.9096 Accuracy 0.5061\n",
      "Epoch 10 Batch 5650 Loss 0.9109 Accuracy 0.5057\n",
      "Epoch 10 Batch 5700 Loss 0.9119 Accuracy 0.5055\n",
      "Saving checkpoint for epoch 10 at C:\\Users\\gaura\\Desktop\\Modern Natural Language Processing in Python\\Transformer\\ckpt\\ckpt-10\n",
      "Time taken for 1 epoch: 6612.93558049202 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T09:24:53.935992Z",
     "start_time": "2020-07-20T09:24:53.839035Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_FR-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T09:25:05.283841Z",
     "start_time": "2020-07-20T09:25:05.236070Z"
    }
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_fr.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T09:25:28.214414Z",
     "start_time": "2020-07-20T09:25:26.639961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: i love you\n",
      "Predicted translation: Je vous souhaite beaucoup d'attention\n"
     ]
    }
   ],
   "source": [
    "translate(\"i love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
